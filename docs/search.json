[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jared Petry",
    "section": "",
    "text": "Education\n\n\nLaptop Code Masters of Environmental Data Science, 2023\n\n\n\nBren School of Environmental Science and Management, University of California, Santa Barbara\n\n\n\nMicroscope BS in Environmental Studies, 2021\n\n\n\nUniversity of California Santa Barbara\n\n\n\n\n\nExperience\n\n\nBar Chart Fisheries Technician, 2022\n\n\n\nPacific States Marine Fisheries Commission\n\n\n\nFish Sustainability Coordinator and Seafood Retailer, 2021\n\n\n\nKanaloa Seafood, Santa Barbara\n\n\n\n\nMagnifying glass Biodiversity Analysis Intern, 2021\n\n\n\nMarine Biodiversity Observation Network\n\n\n\nTree Restoration Intern, 2019-2021\n\n\n\nCheadle Center for Biodiversity and Ecological Restoration"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Jared is a student of the Masters of Environmental Data Science program at UC Santa Barbara. He obtained his environmental studies bachelors of science degree in 2021 at UC Santa Barbara as well, with a focus on ecology. After graduating, Jared spent a year working in both sustainable seafood and salmonid restoration. Jared is interested in applying his growing data science knowledge to creating models that imitate natural systems in order to understand our rapidly changing world. Specifically, he is interested in marine habitat restoration, fisheries and aquaculture science, environmental justice, and the effects of coastal development on marine ecosystems and recreational activities. While believing that our future depends on the health of our oceans, Jared is optimistic that responsible ocean aquaculture can sustain our growing demand for healthy seafood protein. Jared can usually be found surfing and exploring novelty, remote beaches around Santa Barbara County. In the future, he hopes to learn more about the impact of climate change on recurring weather patterns, longshore sand flow, and coastal systems in general."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "“Determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of marine organims”\n\n\n\n\n\n\nFeb 1, 2023\n\n\nJared Petry\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n“Using a dataset with a variety of physical and atmospheric habitat variables train a boosted tree classification model the predict the presence or absence of the short finned eel”\n\n\n\n\n\n\nFeb 1, 2023\n\n\nJared Petry\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n“Estimating the number of homes in Houston that lost power as a result of the first two storms and investigating if socioeconomic factors are predictors of communities recovery from a power outage.”\n\n\n\n\n\n\nFeb 1, 2023\n\n\nJared Petry\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n“Can we use buoy data in R to determine whether or not there are long run trends in wave height, period, and direction? What role does the El Niño Southern Oscillation (ENSO) have in determining patterns in these variables?”\n\n\n\n\n\n\nDec 1, 2022\n\n\nJared Petry\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Posts/stats_buoy_project/index.html",
    "href": "Posts/stats_buoy_project/index.html",
    "title": "Statistical Time-series Analysis of Data from Harvest Buoy",
    "section": "",
    "text": "Background\nLiving in Santa Barbara can be tricky from a surfer’s perspective.  When I moved here from San Diego, I learned that it takes far more diligence and knowledge of reading meteorological data in order to score some fun waves here.  If you really know what you’re doing, you can even get some great spots all to yourself some days. \nLately, I have been hearing from the older locals that we have had the worst three winters in a row that they can remember. As I learned more about climate change in my undergraduate career alongside my own surfing research, I began to wonder if there’s a connection between climate change and surfing opportunities.  Is this all in our heads as frustrated SB surfers? Could the increasing variability of storms due to climate change affect the surf in Santa Barbara?\nThis short blog post will focus on wave prevalence and attributes over time due to patterns in storm activity.  I would like to use R to find out whether there has been a trend or pattern in surf availability based on time series buoy data using classical decomposition. If possible, I would also like to link this to the ENSO cycles.\n\n\n\nMotivation\nMost, if not all adventure sports are being somewhat impacted by climate change due to their dependency on wind, rainfall, temperature, swell, snowmelt, and other climate variables.  We have already seen effects of increased climate variability on the adventure tourism industry, specifically in snow sports (Buckley, 2017).  But how does climate change relate to surfing? Rising sea level paired with anthropogenic variables such as coastal development and depleted sand banks (from upstream hydroelectric dams) combine to exacerbate coastal erosion. This can alter underwater bathymetry that creates the waves we salty people depend on (Arroyo, 2019).  Many surfing locations all over California have been both tragically lost and accidentally created at the hand of human activity.  \nWithin the realm of surf frequency and quality, a key predictor of any given year’s surfing potential in California is the El Niño Southern Oscillation (ENSO) status.  During El Niño years, the storm track over the North Pacific is usually enhanced, and we see swells travel from straight West through the Santa Barbara Channel. During strong events, California sees especially significant surf.  During La Niña, blocking high pressure over the pacific during winter typically limits tropical swells in the eastern pacific, and shifts the storm track to the North, causing many swells to miss the Santa Barbara Channel (Surfline, 2020).  \nThis winter (2022-23), there is a 76 percent chance that we will be experiencing our third La Niña even in a row, which is relatively rare (Johnson, 2022).  Climate change is causing some strange anomalies in the ENSO cycle.  According to National Oceanic and Atmospheric Association (NOAA) researcher Michael McPhaden, “Extreme El Niño and La Niña events may increase in frequency from about one every 20 years to one every 10 years by the end of the 21st century under aggressive greenhouse gas emission scenarios.” (research.noaa.gov).  The last extreme ENSO event that we have had was the winter of 2015-2016, when a brutal stream of storms battered the Santa Barbara coastline, causing considerable damage, and amazing surf.\n\n\nAbout the data\n\n\nShow Code\nknitr::include_graphics(\"images/buoy_pic.png\")\n\n\n\n\n\nI used data from the buoy that I look at every day that guides all my surf decision making, the Harvest Buoy off Point Conception.  It is owned by the Scripps Institute of Oceanography and its data is distributed by NOAA.  This buoy has been up and running since 2004, but the data has a few large gaps (I think because of maintenance or malfunction reasons) up until 2010.  From 2010 onward, there are two observations every single hour of every day until this moment.  The measurements that I am interested in are wave height, dominant wave period - basically wavelength, average wave period, and swell direction.  \nThere are seemingly many ways to retrieve this data, but the one that I was able to get to work was through this link:\nhttps://www.ndbc.noaa.gov/station_history.php?station=46218\nYou can access a description of this data and variables through this link: https://www.ndbc.noaa.gov/measdes.shtml\nThe published data is in text files, with fields separated by spaces. I used the base R read.table() function to read the measurements into a data frame.  With this function I still had to do some renaming and matching of the columns but it left the data intact.\nI ended up with this dataframe below which has observations of wave height, dominant period, average period, peak swell direction, and temperature. The variables that I felt were relevant to this project were wave height (feet), swell direction (degrees), and dominant period (seconds).\n\n\nShow Code\nlibrary(dplyr) \nlibrary(janitor) \nlibrary(here) \nlibrary(tidyverse) \nlibrary(feasts)\nlibrary(readr)\nlibrary(tufte)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(xts)\nlibrary(tsibble)\n\ndf_2010 <- read.table(\"data/txt_files/46218h2010.txt\")\ndf_2011 <- read.table(\"data/txt_files/46218h2011.txt\")\ndf_2012 <- read.table(\"data/txt_files/46218h2012.txt\")\ndf_2013 <- read.table(\"data/txt_files/46218h2013.txt\")\ndf_2014 <- read.table(\"data/txt_files/46218h2014.txt\")\ndf_2015 <- read.table(\"data/txt_files/46218h2015.txt\")\ndf_2016 <- read.table(\"data/txt_files/46218h2016.txt\")\ndf_2017 <- read.table(\"data/txt_files/46218h2017.txt\")\ndf_2018 <- read.table(\"data/txt_files/46218h2018.txt\")\ndf_2019 <- read.table(\"data/txt_files/46218h2019.txt\")\ndf_2020 <- read.table(\"data/txt_files/46218h2020.txt\")\ndf_2021 <- read.table(\"data/txt_files/46218h2021.txt\")\n\nall_wave_df <- rbind(df_2010, df_2011, df_2012, df_2013, df_2014,\n                     df_2015, df_2016, df_2017, df_2018, df_2019, \n                     df_2020, df_2021)\n\nwaves_renamed <- all_wave_df |> \n  rename(year = V1, \n         month = V2, \n         day = V3, \n         hour = V4, \n         minute = V5, \n         wave_height = V9, \n         dom_period = V10,\n         av_period = V11, \n         peak_direction = V12, \n         temp = V15) \n\nwaves_clean <- waves_renamed |> \n  select(year, month, day, hour, minute, wave_height, dom_period,\n         av_period, peak_direction, temp)\nhead(waves_clean) \n\n\n  year month day hour minute wave_height dom_period av_period peak_direction\n1 2010     1   1    0     13        1.65      12.50      8.69            287\n2 2010     1   1    0     43        1.58      12.50      8.67            280\n3 2010     1   1    1     13        1.55      12.50      8.93            290\n4 2010     1   1    1     43        1.62      11.76      9.04            283\n5 2010     1   1    2     13        1.57      12.50      8.63            300\n6 2010     1   1    2     43        1.49      13.33      8.36            273\n  temp\n1 14.2\n2 14.2\n3 14.1\n4 14.1\n5 14.2\n6 14.1\n\n\nTo create a quick visualization of what these variables look like, I calculated the mean of each one per year and made these very simple plots:\n\n\nShow Code\n#--- compare yearly mean wave height \nyearly_mean_wvht <- waves_clean |> \n  group_by(year) |> \n  summarize(wave_height = mean(wave_height)) \n#plot(yearly_mean_wvht)  \n\n#--- compare yearly mean dominant period\nyearly_mean_dom_period <- waves_clean |> \n  group_by(year) |> \n  summarize(period = mean(dom_period))\n#plot(yearly_mean_dom_period) \n\n#--- compare yearly mean swell direction\nyearly_mean_direction <- waves_clean |> \n  group_by(year) |> \n  summarize(direction = mean(peak_direction))\n#plot(yearly_mean_direction)  \n\n#--- Plot the three variables in a 1x3 row\nplot(yearly_mean_wvht, \n     main = \"Average wave height by year\") \n\n\n\n\n\nShow Code\nplot(yearly_mean_dom_period, \n     main = \"Average period by year\") \n\n\n\n\n\nShow Code\nplot(yearly_mean_direction, \n     main = \"Average swell direction by year\") \n\n\n\n\n\nIf you add a linear regression line to the wave height variable across years, you get this slightly concerning downward trend line. What could this mean? We know there are many factors playing a role in the distribution of wave height year to year, specifically seasonality and the El Niño Southern Oscillation cycles. To explore this relationship further, I would like to isolate seasonality and extract a long-run trend from this time series data using classical additive decomposition.\n\n\nShow Code\n#--- add an lm trend line to our wave height averages plot using ggplot\nggplot(data = yearly_mean_wvht, aes(x = year, y = wave_height)) + \n  geom_point(col = \"blue\") + \n  geom_smooth(method = lm, col = \"red\") +\n  labs(title = \"Average wave height by year with linear regression\")\n\n\n\n\n\n\n\nDecomposition\nThe North Pacific comes to life in wintertime, and goes back to sleep in the summer… for the most part. When dealing with very seasonal data like this it can be useful to create a classical additive decomposition model. This model will decompose the data into seasonal, random, and trend components like this: \\[y_t = S_t + T_t + R_t\\]\nAfter we have separated these components we can plot them to compare which significantly impact our data. The classical decomposition model library function requires fixed and exact periodic intervals between data measurements.  The original wave data was taken approximately every 30 minutes and was somewhat regular, but not exact.  So, I converted the measurements to a daily median (more resistant to outliers).  There were still some gaps in the data, so I used tsibble::fill_gaps() to populate the gaps with “na” and tidyr::fill() to convert the “na” values to the last good measurement. After feeding this through our decomposition model, you can see plots generated below for our variables of interest.\n\n\nShow Code\n#--- Add in ISOdate column to use in tsibble index\nwaves_clean$dt <- ISOdate(\n    waves_clean$year,\n    waves_clean$month,\n    waves_clean$day,\n    waves_clean$hour,\n    waves_clean$minute,\n    0\n) |> na.omit()\n\n#--- Convert to time series tibble\nwave_ts = as_tsibble(waves_clean, index=\"dt\") \n\n#--- Create a regular fixed-interval time series by aggregating daily wave\n# height as the mean height for each day.  Also, some days were missing,\n# could be due to wrong days in a month, like Feb, so using fill_gaps() to\n# add any needed rows and fill() to use prior measurement for the gaps.\n\nwave_regular = wave_ts %>%\n  index_by(index = ~ as_date(.)) %>% \n  summarise(\n    wave_height = median(wave_height),\n    dom_period = median(dom_period),\n    av_period = median(av_period),\n    peak_direction = median(peak_direction), \n    temp = median(temp)\n  ) %>%\n  fill_gaps() %>%\n  tidyr::fill(wave_height, .direction = \"down\") %>%\n  tidyr::fill(dom_period, .direction = \"down\") %>%\n  tidyr::fill(av_period, .direction = \"down\") %>%\n  tidyr::fill(peak_direction, .direction = \"down\") %>%\n  tidyr::fill(temp, .direction = \"down\")\n\n# Now, we have a \"regular\" fixed-interval tsibble with median measurements per day, with column 'index'\n\n#--- Classical addititve decomposition of wave height\nwave_regular %>% \n  model(classical_decomposition(wave_height ~ season(\"1 year\"), \n                                type = \"additive\")) %>% \n  components() %>%\n  autoplot() +\n  labs(title = \"Classical addititve decomposition of wave height\")\n\n\n\n\n\nShow Code\n#--- Classical addititve decomposition of dominant wave period\nwave_regular %>% \n  model(classical_decomposition(dom_period ~ season(\"1 year\"), \n                                type='additive')) %>%\n  components() %>%\n  autoplot() + labs(title = \"Classical addititve decomposition of dominant period\")\n\n\n\n\n\nShow Code\n#--- Classical addititve decomposition of average wave period\nwave_regular %>% \n  model(classical_decomposition(av_period ~ season(\"1 year\"), \n                                type='additive')) %>%\n  components() %>%\n  autoplot() + labs(title = \"Classical addititve decomposition of average period\")\n\n\n\n\n\nShow Code\n#--- Classical addititve decomposition of peak swell direction\nwave_regular %>% \n  model(classical_decomposition(peak_direction ~ season(\"1 year\"), \n                                type='additive')) %>%\n  components() %>%\n  autoplot() + labs(title = \"Classical addititve decomposition of peak swell direction\")\n\n\n\n\n\n\n\nDecomposition Analysis\nAs you can tell looking at these decomposition plots, there’s no way to make out a definitive trend in any of the variables, whether they are going up or down over time. Additionally, this is a very short time period in the grand scheme of climate change over time since the industrial revolution. Perhaps we could visualize more meaningful relationships over time if these measurements had taken place since the 1940s or so.\nHowever, we do see an interesting spike in wave height and period during a time period around 2016. We also see a sharp drop in swell direction to about an average of 280 degrees. These conditions are all very favorable to surfing in Santa Barbara. This time period around 2016 also aligns with the very strong El Niño event that took place that year. Here is a useful graphic of El Niño oscillations over our time period of interest (Null, 2022).\n\n\n\n\n\n\n\nAnother Question\nSince we see the variables changing during this El Niño event, I would like to compare our wave height, period, and direction during a strong El Niño event with that of a strong La Niña event. Since there is only one of each happening within the 2010-2021 period that we have buoy data for, I will compare the 2010-11 La Niña event (June 1, 2010 - May 31, 2011) with the 2015-16 El Niño event (April 1, 2015 - March 31, 2016).\nFirst, I’ll subset our time series dataframe into just those time periods, with an added enso column that will either say “el_nino” or “la_nina”. Then I will be able to use the stats::t.test() function to determine whether the difference in means of our variables of interest differ statistically between strong El Niño and La Niña events.\n\n\nShow Code\n#--- create la nina filter: june 1, 2010- may 31 2011 and create an enso column that says that it was la nina\n\nla_nina <- wave_regular |> \n  filter(between(index, as.Date(\"2010-06-01\"), as.Date(\"2011-05-31\"))) \nla_nina$enso <- \"la_nina\"\n  \n#--- create el nino filter: june 1, 2010- may 31 2011 and create an enso column that says that it was el nino\n\nel_nino <- wave_regular |> \n  filter(between(index, as.Date(\"2015-04-01\"), as.Date(\"2016-03-31\"))) \nel_nino$enso <- \"el_nino\"\n\n#--- visualize the means of our variables \ncomparison_chart <- data.frame(enso = c(\" 2010-11 la nina\", \"2015-16 el nino\"), \n                               mean_height = c(mean(la_nina$wave_height), \n                                               mean(el_nino$wave_height)),\n                               mean_period = c(mean(la_nina$dom_period), \n                                               mean(el_nino$dom_period)),\n                               mean_direction = c(mean(la_nina$peak_direction),\n                                                  mean(el_nino$peak_direction)))\n                               \ncomparison_chart\n\n\n              enso mean_height mean_period mean_direction\n1  2010-11 la nina    2.203219    11.87426       287.4562\n2  2015-16 el nino    2.309850    12.49772       281.9139\n\n\n\nSignificance Tests\nIn order to test whether these two ENSO events are significantly different in wave height, period, and swell direction, I will run three separate t-tests between the two time periods.\nWave height-\nNull Hypothesis: There is no difference in the mean wave height of the 2010-11 La Niña event and the 2015-16 El Niño event.\nAlternative Hypothesis: There is a difference in the mean wave height of the 2010-11 La Niña event and the 2015-16 El Niño event.\n\n\nShow Code\n#--- join these dataframes together to run t-test\n\ncomparison_df <- bind_rows(la_nina, el_nino)\n\n#--- run t-test to see if enso status really affects our variables\n\n#--- WAVE HEIGHT\nt.test(wave_height~enso, data = comparison_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  wave_height by enso\nt = 1.6275, df = 673.93, p-value = 0.1041\nalternative hypothesis: true difference in means between group el_nino and group la_nina is not equal to 0\n95 percent confidence interval:\n -0.02201407  0.23527517\nsample estimates:\nmean in group el_nino mean in group la_nina \n             2.309850              2.203219 \n\n\nWe fail to reject the null hypothesis at the alpha = 0.05 significance level.\nDominant Period-\nNull Hypothesis: There is no difference in the mean dominant period of the 2010-11 La Niña event and the 2015-16 El Niño event.\nAlternative Hypothesis: There is a difference in the mean dominant period of the 2010-11 La Niña event and the 2015-16 El Niño event.\n\n\nShow Code\n#--- PERIOD\nt.test(dom_period~enso, data = comparison_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  dom_period by enso\nt = 3.0192, df = 720.98, p-value = 0.002624\nalternative hypothesis: true difference in means between group el_nino and group la_nina is not equal to 0\n95 percent confidence interval:\n 0.2180466 1.0288700\nsample estimates:\nmean in group el_nino mean in group la_nina \n             12.49772              11.87426 \n\n\nWe reject the null hypothesis at the alpha = 0.05 significance level.\nPeak Swell Direction-\nNull Hypothesis: There is no difference in the mean peak swell direction of the 2010-11 La Niña event and the 2015-16 El Niño event.\nAlternative Hypothesis: There is a difference in the mean peak swell direction of the 2010-11 La Niña event and the 2015-16 El Niño event.\n\n\nShow Code\n#--- DIRECTION\nt.test(peak_direction~enso, data = comparison_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  peak_direction by enso\nt = -1.9964, df = 728.15, p-value = 0.04627\nalternative hypothesis: true difference in means between group el_nino and group la_nina is not equal to 0\n95 percent confidence interval:\n -10.99248229  -0.09197763\nsample estimates:\nmean in group el_nino mean in group la_nina \n             281.9139              287.4562 \n\n\nWe reject the null hypothesis at the alpha = 0.05 significance level.\n\n\n\nDiscussion\nIt is interesting to see that with a significance value of alpha = 0.05, the difference in wave height was not statistically significant (but still close). However, the mean period was significantly smaller for the La Niña period than the El Niño, AND the mean swell direction was significantly steeper (more northerly, and less favorable for surfing in Santa Barbara) during the La Niña event. This lends support to the observation that El Niño years have bigger waves, longer periods, and a better swell direction (closer to 270 degrees) than La Niña years.\nHowever, this is still relatively inconclusive, because no two ENSO cycles are exactly alike and we only have one of each here to compare. It is likely that different ENSO events have slightly different qualities and it would be irresponsible to generalize that just from comparing two of them we have uncovered some absolute truths. Nonetheless, comparing these two events did line up with what I outlined in the “motivation” section above. Due to the altering of the pacific storm track during an El Niño event, we see more waves coming straight into the SB channel (~270 degrees, straight West) without having to wrap around Point Conception to the North.\nIn further research, I would look into data from the West Santa Barbara buoy. This location is far more protected from north and south swells, which could be useful for more specific stats questions pertaining to just west swells. However, the refraction of swell around the islands and Point Conception sucks a lot of energy out of the swell which could decrease the variability in our data, therefore making statistical inferences more subtle.\nIn conclusion, that results I got here were probably known by many experts and most surfers via their own anecdotal evidence. Either way, it was still really cool to find evidence through my favorite buoy that supported my own real-life observations.\n\n\nReferences\nArroyo, M., Levine, A., & Espejel, I. (2019). A transdisciplinary framework proposal for surf break conservation and management: Bahía de Todos Santos World Surfing Reserve. Ocean & Coastal Management, 168, 197–211. https://doi.org/10.1016/j.ocecoaman.2018.10.022\nBuckley, R. (2017). Perceived Resource Quality as a Framework to Analyze Impacts of Climate Change on Adventure Tourism: Snow, Surf, Wind, and Whitewater. Tourism Review International, 21(3), 241–254. https://doi.org/10.3727/154427217X15022104437729\nEl Nino, La Nina, ENSO and What They Mean to Your Surf. (2020, April 17). Surfline. https://www.surfline.com/surf-news/el-nino-la-nina-enso-what-they-mean-to-your-surf/82991\nGoldman, S. (2017, March 1). Storms, Powerful Waves Have Eaten Away Santa Barbara County Coastlines to Historic Levels. https://www.noozhawk.com/article/storms_waves_coastal_erosion_santa_barbara_county_historic_levels\nHow will climate change change El Niño and La Niña? - Welcome to NOAA Research. (2020, November 9). How Will Climate Change Change El Niño and La Niña? https://research.noaa.gov/article/ArtMID/587/ArticleID/2685/New-research-volume-explores-future-of-ENSO-under-influence-of-climate-change\nJohnson, N. (2022, November 22). Another winter in La Niña’s grip? – November update to NOAA’s 2022-23 Winter Outlook | NOAA Climate.gov. http://www.climate.gov/news-features/blogs/another-winter-la-ni%C3%B1a%E2%80%99s-grip-%E2%80%93-november-update-noaas-2022-23-winter-outlook\nNull, J. (2022, October 1). El Niño and La Niña Years and Intensities. https://ggweather.com/enso/oni.htm"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html",
    "href": "Posts/marine_aquaculture_geospatial/index.html",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "",
    "text": "source: https://fieldnotesjournal.org/new-blog/apearlintheroughoysteraquacultureandhowitworks"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#overview",
    "href": "Posts/marine_aquaculture_geospatial/index.html#overview",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Overview",
    "text": "Overview\nMarine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al.\nFor this small project, we will determine which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nsea surface temperature: 11-30 deg C\ndepth: 0-70 meters below sea level\n\n\nUtilized skills:\n\ncombining vector raster data\nresampling raster data\nmasking raster data\nmap algebra"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#data",
    "href": "Posts/marine_aquaculture_geospatial/index.html#data",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).2\n\n\nExclusive Economic Zones\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org."
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#prepare-data",
    "href": "Posts/marine_aquaculture_geospatial/index.html#prepare-data",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Prepare data",
    "text": "Prepare data\nTo start, we need to load all necessary data and make sure it has the coordinate reference system.\nWest coast regions:\n\n# Read in the shapefile for the West Coast EEZ (`wc_regions_clean.shp`)\n\nwc_regions <- st_read(here(file_path, \"data/wc_regions_clean.shp\"))\n\nReading layer `wc_regions_clean' from data source \n  `/Users/jaredpetry/Documents/MEDS/quarto_website/jaredbpetry.github.io/Posts/marine_aquaculture_geospatial/data/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\nplot(wc_regions$geometry)\n\n\n\n\nSea surface tmeperature:\n\n# Read in sea surface temperature rasters and combine them into a raster stack\n# use list.files() to read in our data \n#--- you start with a bunch of tif files that you want to stack\n#--- I created a list of just the ones starting with the letter \"a\" to get the sst rasters I wanted\nfile_list <- list.files(path = \"data/\", pattern = \"^[a]\", full.names = TRUE)\n#--- now read them in using rast... this will create a spatraster with multiple layers\n#--- (this isn't an actual raster stack so let's see if this works.. terra calls them the same thing)\nsst_spatrast <- rast(file_list)\nplot(sst_spatrast$average_annual_sst_2009)\n\n\n\n#--- at this point you get a crs that says epsg4326 AND epsg9122 so we'll have to change that\n\nBathymetry raster:\n\n# Read in bathymetry raster (`depth.tif`)\ndepth <- rast(here(file_path, \"data/depth.tif\"))\nplot(depth)\n\n\n\n\nReprojecting data so they match in their coordinate reference system:\n\n#--- depth raster is in lon/lat wgs84 EPSG:4326\n#--- wc_regions polygons in wgs84\n#--- sst_spatraster says lon/lat wgs84\n#depth <- st_transform(depth, crs = crs(sst_spatrast))\n\nset.crs(sst_spatrast, \"EPSG:4326\")\nset.crs(depth, \"EPSG:4326\")\nst_transform(wc_regions, crs = crs(depth)) #--- great now they are same crs\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key      area_m2 rgn_id  area_km2\n1              Oregon      OR 179994061293      1 179994.06\n2 Northern California    CA-N 164378809215      2 164378.81\n3  Central California    CA-C 202738329147      3 202738.33\n4 Southern California    CA-S 206860777840      4 206860.78\n5          Washington      WA  66898309678      5  66898.31\n                        geometry\n1 MULTIPOLYGON (((-123.4318 4...\n2 MULTIPOLYGON (((-124.2102 4...\n3 MULTIPOLYGON (((-122.9928 3...\n4 MULTIPOLYGON (((-120.6505 3...\n5 MULTIPOLYGON (((-122.7675 4..."
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#process-data",
    "href": "Posts/marine_aquaculture_geospatial/index.html#process-data",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Process data",
    "text": "Process data\nNext, we need process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\nFind the mean SST from 2008-2012\n\nmean_sst_spatrast <- terra::app(sst_spatrast, mean) \n\nConvert SST data from Kelvin to Celsius\n\nmean_sst_C <- mean_sst_spatrast - 273.15\n\nCrop depth raster to match the extent of the SST raster\n\ndepth_cropped <- extend(depth, mean_sst_C)\n\nResample the NPP data to match the resolution of the SST data using the nearest neighbor approach\n\ndepth_cropped_resampled <- resample(depth_cropped, mean_sst_C, \n                                    method = \"near\")\n\nCheck that the depth and SST match in resolution, extent, and coordinate reference system… can the rasters be stacked?\n\n#--- try to stack the depth raster and the SST raster:\ndepth_and_sst <- raster::stack(c(depth_cropped_resampled, mean_sst_C))\n#--- yes! they can be stacked"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#find-suitable-locations",
    "href": "Posts/marine_aquaculture_geospatial/index.html#find-suitable-locations",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Find suitable locations",
    "text": "Find suitable locations\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth.\nReclassify SST and depth data into locations that are suitable for oysters. We will do this by setting suitable values to 1 and unsuitable values to NA\nThe first plot will show suitable depth for oysters and the second will showsuitable sea surface temperature.\n\n#--- for oysters, we need 11 <= sst(C) <= 30\n#------ and 0 <= depth <= 70\n\n#--- set up reclassification matrices \n\ndepth_rcl <- matrix(c(-8000, -70, NA,\n                    -70, 0, 1,\n                    0, 5000, NA), \n                 ncol = 3, byrow = TRUE)\n\nsst_rcl <- matrix(c(-Inf, 11, NA, \n                    11, 30, 1, \n                    30, Inf, NA), \n                 ncol = 3, byrow = TRUE)\n\n#--- use these to classify your rasters\n\nsuitable_sst <- classify(mean_sst_C, rcl = sst_rcl)\n\nsuitable_depth <- classify(depth_cropped_resampled, rcl = depth_rcl)\n\n#--- Plot what you just made for each \n\nraster::plot(suitable_depth, col = \"blue\") \n\n\n\nraster::plot(suitable_sst, col = \"blue\")\n\n\n\n\nFind locations that satisfy both SST and depth conditions\n- create an overlay using the lapp() function multiplying cell values - we will use terra::lapp()\n\n#--- we want to multiply cell values so that with any NA in either layer, result will also be NA, with two 1s, the result will also be a 1\n\nfun = function(x,y) {\n  return(x*y)\n}\n\noyster_habitat <- lapp(c(suitable_depth, suitable_sst), fun)\nplot(oyster_habitat, col = \"blue\")\n\n\n\n\n\nDetermine the most suitable EEZ\nWe want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nselect suitable cells within West Coast EEZs\nfind area of grid cells\nfind the total suitable area within each EEZ\nfind the percentage of each zone that is suitable\n\n\n#--- remember, our eez regions are the wc_regions we reprojected earlier\n\n# select suitable cells within West Coast EEZs\n#----first rasterize the wc_regions data to create rast_eez\nrast_regions <- terra::rasterize(wc_regions, oyster_habitat, field = \"rgn\")\n\n# compute the area covered by the raster cells of suitable habitat\n#--- terra::cellSize() will compute the area covered by each individual cell\narea_habitat <- cellSize(oyster_habitat, unit = \"km\", transform = TRUE)\n\n#--- create mask that will display the suitable habitat separated into the different eez regions\nmask <- mask(rast_regions, oyster_habitat)\n\n#--- terra::zonal() will compute summaries of values of a spatraster defined by the \"zones\" of a different spatraster.  We will use this to compute the suitable area \nhabitat_area <- terra::zonal(area_habitat, mask, sum)\n\n#--- use left_join() to create a dataframe that contains both the suitable area by region and the percentage of habitat area out of the total area for that region\nhabitat_by_region_df <- left_join(wc_regions, habitat_area, by = \"rgn\")  \nhabitat_by_region_df$area_percent = (habitat_by_region_df$area / habitat_by_region_df$area_km2)*100\n\n#--- show results in a table\nprint_df <- habitat_by_region_df |> \n  terra::as.data.frame() |> \n  dplyr::select(rgn, area, area_percent) |> \n  dplyr::rename(\"Oyster Habitat Area\" = area, \n                \"EEZ region\" = rgn, \n                \"Habitat Percent of Total Region Area\" = area_percent) \nprint_df\n\n           EEZ region Oyster Habitat Area Habitat Percent of Total Region Area\n1              Oregon           1074.2720                            0.5968374\n2 Northern California            178.0268                            0.1083028\n3  Central California           4069.8766                            2.0074530\n4 Southern California           3757.2849                            1.8163351\n5          Washington           2378.3137                            3.5551178"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#visualize-results",
    "href": "Posts/marine_aquaculture_geospatial/index.html#visualize-results",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Visualize results",
    "text": "Visualize results\nNow that we have results, we need to present them!\nWe will create the following maps:\n\ntotal suitable area by region\npercent suitable area by region\n\n\n#--- map of total area \narea_map <- tm_shape(habitat_by_region_df) + \n  tm_polygons(col = \"area\",\n              palette = rev(hcl.colors(3, \"BluGrn\")),\n              title = \"Habitat Area (square km)\",\n              legend.reverse = TRUE) +\ntm_shape(oyster_habitat) +\n  tm_raster(title = \"Habitat\") +\ntm_layout(legend.outside = TRUE,\n          main.title.size = 1,\n          main.title = \"Suitable habitat area for oysters by EEZ region\")\n\narea_map\n\n\n\n#--- map of percentage\npercent_map <- tm_shape(habitat_by_region_df) +\n  tm_polygons(col = \"area_percent\",\n              palette = rev(hcl.colors(3, \"BluGrn\")),\n              title = \"Percentage\") +\ntm_shape(oyster_habitat) +\n  tm_raster(title = \"Habitat\") +\ntm_layout(legend.outside = TRUE,\n          main.title.size = 1,\n          main.title = \"Suitable habitat for oysters: percentage of total EEZ region\",\n          frame = T)\npercent_map"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#broaden-the-workflow",
    "href": "Posts/marine_aquaculture_geospatial/index.html#broaden-the-workflow",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Broaden the workflow!",
    "text": "Broaden the workflow!\nNow that we’ve worked through the suitable habitat areas for one group of species, let’s update our workflow to work for other species. We will do this by creating a function that would allow you to reproduce your results for other species. It will be able to do the following:\n\naccept temperature and depth ranges and species name as inputs\ncreate maps of total suitable area and percent suitable area per EEZ with the species name in the title\n\n\nfind_suitable_habitat <- function(spp_name, temp_min, temp_max, depth_min, depth_max) {\n  depth_rcl <- matrix(c(-Inf, depth_min, NA, \n                               depth_min, depth_max, 1, \n                               depth_max, Inf, NA), \n                              ncol = 3, byrow = TRUE)\n  sst_rcl <- matrix(c(-Inf, temp_min, NA, \n                             temp_min, temp_max, 1, \n                             temp_max, Inf, NA), \n                           ncol = 3, byrow = TRUE)\n  suitable_sst <- classify(mean_sst_C, rcl = sst_rcl)\n  suitable_depth <- classify(depth_cropped_resampled, rcl = depth_rcl)\n  oyster_habitat <- lapp(c(suitable_depth, suitable_sst), fun)\n  rast_regions <- terra::rasterize(wc_regions, oyster_habitat, field = \"rgn\")\n  area_suitable <- cellSize(oyster_habitat, unit = \"km\", transform = TRUE)\n  mask <- mask(rast_regions, oyster_habitat)\n  area_per_zone <- terra::zonal(area_suitable, mask, sum)\n  df_habitat_rgn <- left_join(wc_regions, area_per_zone, by = \"rgn\") \n  df_habitat_rgn$area_percent <- (df_habitat_rgn$area / df_habitat_rgn$area_km2)*100\n  #--- map of total area \n  area_and_percent_map <- tmap_arrange(tm_shape(df_habitat_rgn) + \n    tm_polygons(col = \"area\",\n                palette = rev(hcl.colors(3, \"BluGrn\")),\n                title = \"Area(km sq.)\",\n                legend.reverse = TRUE) +\n    tm_shape(oyster_habitat) +\n      tm_raster(title = \"Habitat\") +\n    tm_layout(legend.outside = TRUE,\n              legend.title.size = 0.8,\n              legend.text.size = 0.5,\n              main.title.size = 0.8,\n              main.title = paste0(spp_name, \" habitat by EEZ region\")),\n    percent_map <- tm_shape(df_habitat_rgn) +\n      tm_polygons(col = \"area_percent\",\n                  palette = rev(hcl.colors(3, \"BluGrn\")),\n                  title = \"Percentage\") +\n    tm_shape(oyster_habitat) +\n      tm_raster(title = \"Habitat\") +\n    tm_layout(legend.outside = TRUE,\n              legend.title.size = 0.8,\n              legend.text.size = 0.5,\n              main.title.size = 0.8,\n              main.title = paste0(spp_name, \" habitat percentage of EEZ region\"),\n              frame = T))\n  area_and_percent_map\n}\n#--- test the function (arbitrary values) \nfind_suitable_habitat(spp_name = \"spp of interest\", 5, 15, -100, 0)\n\n\n\n\nAnyone who wants to reproduce this workflow can do so easily! Run your function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase. Remember, we are thinking about the potential for marine aquaculture, so these species should have some reasonable potential for commercial consumption.\nI chose the American Lobster because it is a highly sought after food that could return lots of profit if grown in the right place. I used to work at a fish market and it was extremely hard to get lobster because it is so hard to get nowadays! The temperature range is 11-19 degC and the depth range is sea level to 480 meters deep.\n\nfind_suitable_habitat(spp_name = \"American lobster\", 11, 19, -480, 0)\n\n\n\n\n\nSB depth data… just for fun\nCan I do a SB analysis with the depth data? I was going to try this to see bathymetry data for surf spot intel… however the data wasn’t high enough resolution. Oh well it still looks cool at least\n\nrast_to_crop <- depth\nlat_up <- 34.522\nlat_down <- 34.085\nlon_up <- -120.223\nlon_down <- -119.171\n#--- formatting for ext(): -180, 180, -90, 90 (xmin, xmax, ymin, ymax)\nextent_of_crop <- terra::ext(lon_up, lon_down, lat_down, lat_up)\nsb_depth <- crop(rast_to_crop, extent_of_crop)\n\n#--- also crop the basemap to the same thing\n#sb_basemap <- extend(wc_regions, sb_depth)\n\ntm_shape(sb_depth) + tm_raster()"
  },
  {
    "objectID": "Posts/houston_blackout/index.html",
    "href": "Posts/houston_blackout/index.html",
    "title": "Investigating Socioecomonic Factors in the 2021 Houston Blackout with Geospatial Data",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nFor this assignment, we will:\n- estimate the number of homes in Houston that lost power as a result of the first two storms\n- investigate if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, we link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, we will link our analysis with data from the US Census Bureau.\n\n\n\nload vector/raster data\n\nsimple raster operations\n\nsimple vector operations\n\nspatial joins"
  },
  {
    "objectID": "Posts/houston_blackout/index.html#assignment",
    "href": "Posts/houston_blackout/index.html#assignment",
    "title": "Investigating Socioecomonic Factors in the 2021 Houston Blackout with Geospatial Data",
    "section": "Assignment",
    "text": "Assignment\nBelow is an outline of the steps you should consider taking to achieve the assignment tasks.\n\nFind locations of blackouts\nFor improved computational efficiency and easier inter-operability with sf, I recommend using the stars package for raster handling.\n\n\ncombine the data (5 points)\n\nread in night lights tiles\n\ncombine tiles into a single stars object for each date (2021-02-07 and 2021-02-16)\n\n\nhint: use st_mosaic\n\n\n\n\n\nShow Code\n#--- Load in the raster files for the tiles you want\n\ntile1_0207_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\ntile1_0207 <- read_stars(tile1_0207_path)\n\ntile2_0207_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\ntile2_0207 <- read_stars(tile2_0207_path)\n\ntile1_0216_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\ntile1_0216 <- read_stars(tile1_0216_path)\n\ntile2_0216_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\ntile2_0216 <- read_stars(tile2_0216_path)\n\n#--- Use st_mosaic() to combine tiles for each date\n\nrast_0207 <- st_mosaic(tile1_0207, tile2_0207)\nrast_0216 <- st_mosaic(tile1_0216, tile2_0216)\n\n\n\n\nShow Code\n#--- Visualize the rasters that we created \nplot(rast_0207) \n\n\ndownsample set to 6\n\n\n\n\n\nShow Code\nplot(rast_0216)\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nShow Code\nsfrast_0207 <- st_as_sf(rast_0207) #sf object in wgs84\n\n\n\n\ncreate a blackout mask (10 points)\n\nfind the change in night lights intensity (presumably) caused by the storm\nreclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nassign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\n\n\n\nShow Code\n#--- Get the difference in values for a new raster diff_rast\ndiff_rast <- rast_0216 - rast_0207 #stars object\ndiff_rast_over200 <- diff_rast > 200\ndiff_rast_over200[diff_rast_over200 == 0] = NA\nplot(diff_rast_over200) #stars object\n\n\nWarning in plot.stars(diff_rast_over200): breaks=\"quantile\" leads to a single\nclass; maybe try breaks=\"equal\" instead?\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nvectorize the mask (5 points)\n\nuse st_as_sf() to vectorize the blackout mask\nfix any invalid geometries using st_make_valid\n\n\n\nShow Code\n#--- changing stars object to sf object\nblackout_vect <- st_as_sf(diff_rast_over200) |> \n  st_make_valid()\nplot(blackout_vect)\n\n\n\n\n\n\n\ncrop the vectorized map to our region of interest (10 points)\n\ndefine the Houston metropolitan area with the following coordinates\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nturn these coordinates into a polygon using st_polygon\nconvert the polygon into a simple feature collection using st_sfc() and assign a CRS\n\nhint: because we are using this polygon to crop the night lights data it needs the same CRS\n\ncrop (spatially subset) the blackout mask to our region of interest \nre-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\nShow Code\n#--- create a list of points for your polygon...THE ORDER MATTERS..\n#--- had to add the first point again at the end \n#--- to avoid an error about it not being 'closed'\nhouston_list = list(rbind(c(-96.5, 29), c(-96.5, 30.5), \n                          c(-94.5, 30.5), c(-94.5, 29), \n                          c(-96.5, 29)))  \n\n#--- now form a polygon from those points \nhouston_polygon <- st_polygon(houston_list) # 'polygon'\n\n#--- convert this to an sf object \nhouston_poly_sfc <- st_sfc(houston_polygon, crs = 4326) # 'sfc polygon'\nhouston_poly_sf <- st_as_sf(houston_poly_sfc) # 'sf object'\n\n#--- make spatial subset of the blackout mask with the polygon\nhouston_subset <- st_crop(blackout_vect, houston_poly_sf) # in wgs84\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nShow Code\n#--- convert this subset to EPSG 3083 projection\nhouston_blackout <- st_transform(houston_subset, \"EPSG: 3083\") # in NAD83\n\n\n\n\nexclude highways from blackout mask (10 points)\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\n\n\ndefine SQL query\nload just highway data from geopackage using st_read\nreproject data to EPSG:3083\nidentify areas within 200m of all highways using st_buffer\n\nhint: st_buffer produces undissolved buffers, use st_union to dissolve them\n\nfind areas that experienced blackouts that are further than 200m from a highway\n\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways <- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query)\n\n\nShow Code\n#--- Define SQL query\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n#--- Load highway data from geopackage\nhighways <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_roads_free_1.gpkg\", query = query)\n\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'' from data source `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n\nShow Code\n#--- Set CRS\nhighways3083 <- st_transform(highways, \"EPSG:3083\")\nhighways3083_geom <- highways3083$geom\n\n#--- Create buffer of 200m from highways (I believe that meters is default units for NAD83)**\n#--- highway_buffer is now \"sfc_MULTIPOYGON\"\nhighway_buffer <- st_buffer(highways3083_geom, dist = 200) |> st_union()\n\n#--- Now we can make our mask\nhouston_no_hwy <- houston_blackout[highway_buffer, op = st_disjoint]\n\n\n\n\n\nFind homes impacted by blackouts\n\nload buildings data (10 points)\n\nload buildings dataset using st_read and the following SQL query to select only residential buildings\nhint: reproject data to EPSG:3083\n\n\n\nShow Code\nhomes_query <- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nhomes <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_buildings_a_free_1.gpkg\", query = homes_query) |> \n  st_transform(\"EPSG:3083\")\n\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')' from data source `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n\n\n\nfind homes in blackout areas (20 points)\n\nfilter to homes within blackout areas\ncount number of impacted homes\n\n\n\nShow Code\n#--- filter for the homes that are within the blackout vector (I'm assuming we are using what I just made above for this)\nblackout_homes <- homes[houston_no_hwy, \"geom\"]\n\n#--- count the number of homes in this subset\nprint(nrow(blackout_homes)) #--- 71,233 homes affected\n\n\n[1] 71233\n\n\n\n\n\nInvestigate socioeconomic factors\n\nload ACS data (10 points)\n\nuse st_read() to load the geodatabase layers\ngeometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer\nincome data is stored in the X19_INCOME layer\nselect the median income field B19013e1\nhint: reproject data to EPSG:3083\n\n\n\n\nShow Code\n#--- read geometries layer and change the column to match \ntract <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", \n                      layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") |> \n  mutate(GEOID = GEOID_Data) \n\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\n\nShow Code\n#--- read in the income layer and select columns GEOID and median income\nmedian_income <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                     layer = \"X19_INCOME\", \n                     geometry_column = \"GEOID\") |> \n  select(\"GEOID\", \"B19013e1\") \n\n\nReading layer `X19_INCOME' from data source \n  `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\n\n\ndetermine which census tracts experienced blackouts (10 points)\n\njoin the income data to the census tract geometries\nhint: make sure to join by geometry ID\nspatially join census tract data with buildings determined to be impacted by blackouts\nfind which census tracts had blackouts\n\n\n\n\nShow Code\n#--- left join with the blackout info to get geometries\ntract_w_income <- left_join(tract, median_income, \n                           by = \"GEOID\") |> \n  st_transform(crs = \"EPSG:3083\") |> \n  rename(median_income = B19013e1)\n\n#--- spatially join with buildings that experienced blackouts (blackout_homes)\nblack_tract_vector <- st_join(blackout_homes, tract_w_income, \n                            join = st_within)\n# plot(income_buildings$geometry) \n# plot(blackout_homes)\n\n#--- find which census tracts have homes that experienced blackouts\n#--- to do this, find the unique rows by census tract from the object above\nprint(n_distinct(black_tract_vector$TRACTCE))\n\n\n[1] 629\n\n\nShow Code\n#--- this says that there are 629 unique tracts with homes that experienced blackouts\n\n#--- Make sure this is right with a couple methods:\nblack_tract_unique <- unique(black_tract_vector$TRACTCE)\n#--- also has 629 tracts with blackouts \n\n# create a subset of tracts that had blackouts in them\nblackout_tracts <- tract[tract$TRACTCE %in% \nblack_tract_unique,]\n\n# create a subset of tracts with no blackouts \nunaffected_tracts <- tract[!(tract$TRACTCE %in% black_tract_unique),]\n\n\n\n\ncompare incomes of impacted tracts to unimpacted tracts (10 points)\n\ncreate a map of median income by census tract, designating which tracts had blackouts\n\n\n\nShow Code\n#--- subset all the cencus tracts to houston only:\n#--- follow the same steps as before: crop while in wgs84 and then transform to \n#--- NAD83 after\n\nhouston_all_tracts <- tract_w_income |> \n  st_transform(\"EPSG:4326\") |> \n  st_crop(houston_poly_sf) |> \n  st_transform(\"EPSG:3083\")  \n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nShow Code\n#--- also crop the blackout_tracts object\n\naffected_tract_cropped <- blackout_tracts |> \n  st_transform(\"EPSG:4326\") |> \n  st_crop(houston_poly_sf) |> \n  st_transform(\"EPSG:3083\")\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nShow Code\n#--- same thing for the unnafected tracts \n\nunaffected_tracts_cropped <- unaffected_tracts |> \n  st_transform(\"EPSG:4326\") |> \n  st_crop(houston_poly_sf) |> \n  st_transform(\"EPSG:3083\")\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nShow Code\n#--- Plot all tracts versum the blackout tracts to make sure we're on track:\n\n#plot all the houston tracts \nhouston_all_tracts_map <- tm_shape(houston_all_tracts) +\n  tm_polygons() \nhouston_all_tracts_map\n\n\n\n\n\nShow Code\n#--- Plot just blackout tracts \nblackout_tracts_map <- tm_shape(affected_tract_cropped) +\n  tm_polygons()\nblackout_tracts_map\n\n\n\n\n\nShow Code\n#--- Make a plot to see how that went\nunaffected_tracts_map <- tm_shape(unaffected_tracts_cropped) + \n  tm_polygons() \nunaffected_tracts_map\n\n\n\n\n\n\n\n\nMake the Map! woo :)\nstart with all the tracts and color based on income\n\n\nShow Code\n#--- how are we gonna get the blackout status in the map?\n#--- make a dataframe with just tract, geoid, and median income\n#tract_geoid_income\n\n#--- First let's try making a map of just income and census tract\n\nmedian_income_map <- tm_shape(houston_all_tracts) + \n  tm_fill(col = \"median_income\", palette = \"YlOrRd\") +\n  tm_borders() +\n  tm_layout(legend.outside = T, \n            main.title = \"Median Income by Census tract\",\n            frame = T)\nmedian_income_map\n\n\n\n\n\nShow Code\n#--- Since there's a lot going on here, let's split into two maps to compare the blackout tracts vs. the anaffected tracts \n\n#--- add income data to affected tracts\nblackout_w_income <- st_join(affected_tract_cropped, tract_w_income)\n\n#--- make map of blackout tracts\nblackout_w_income_map <- tm_shape(blackout_w_income) + \n  tm_fill(col = \"median_income\", palette = \"YlOrRd\") +\n  tm_layout(legend.position = c(\"right\", \"top\"),\n            legend.width = .3,\n            legend.height = .2,\n            main.title = \"Income in affected tracts\",\n            main.title.size = 1,\n            frame = T)\n\n#--- add income data to unaffected tracts\nunaffected_w_income <- st_join(unaffected_tracts_cropped, tract_w_income)\n\n#--- make map of unaffected tracts\nunaffected_w_income_map <- tm_shape(unaffected_w_income) + \n  tm_fill(col = \"median_income\", palette = \"YlOrRd\") +\n  tm_layout(legend.show = F, \n            main.title = \"Income in unaffected tracts\",\n            main.title.size = 1,\n            frame = T)\n\ntmap_arrange(blackout_w_income_map, unaffected_w_income_map, \n             ncol = 2, \n             nrow = 1)\n\n\n\n\n\n\nplot the distribution of income in impacted and unimpacted tracts\n\n\n\nShow Code\n#--- make two boxplots next to eachother with income data of tracts to compare blackout vs. non-blackout tracts \n\nboxplot(blackout_w_income$median_income, unaffected_w_income$median_income, \n        names = c(\"affected\", \"unaffected\"))\n\n\n\n\n\n\nwrite approx. 100 words summarizing your results and discussing any limitations to this study\n\nUpon looking at the maps and boxplot I made above, there does not seem to be a very significant difference in the median income between the two groups: those affected by the blackout and those not affected. That really surprised me because I believe that the electrical infrastructure in lower income neighborhoods would have likely been older and more subject to failure while the grid was in a stressful state during this weather event. I would say that a limitation to this study is the fact that the two satellite images that we started with happened to be a bit far apart temporally. I would hypothesize that if we were able to obtain a photo right before the blackout happened, and then right when the blackout was at its most severe, we would have been able to see a larger difference, and possibly seen more significant results. Additionally, the data may have been slightly misleading for the income data because the areas farther from the city seemed to be much less affected, and likely contain very different demographics than those closer to the urban areas of Houston. I noticed that the more rural areas tended to have slightly lower median incomes. It would be interesting to focus on a smaller area, because these large tracts may have skewed our data a bit."
  },
  {
    "objectID": "Posts/houston_blackout/index.html#data",
    "href": "Posts/houston_blackout/index.html#data",
    "title": "Investigating Socioecomonic Factors in the 2021 Houston Blackout with Geospatial Data",
    "section": "Data",
    "text": "Data\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nRoads\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\n\nHouses\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nWe use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata.\n\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "Posts/houston_blackout/index.html#project",
    "href": "Posts/houston_blackout/index.html#project",
    "title": "Investigating Socioecomonic Factors in the 2021 Houston Blackout with Geospatial Data",
    "section": "Project",
    "text": "Project\nLoad and visualize aerial raster files during the blackout and after:\n\n#--- Load in the raster files for the tiles you want\n\ntile1_0207_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\ntile1_0207 <- read_stars(tile1_0207_path)\n\ntile2_0207_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\ntile2_0207 <- read_stars(tile2_0207_path)\n\ntile1_0216_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\ntile1_0216 <- read_stars(tile1_0216_path)\n\ntile2_0216_path <- file.path(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data\", \"VNP46A1\", \n                            \"VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\ntile2_0216 <- read_stars(tile2_0216_path)\n\n#--- Use st_mosaic() to combine tiles for each date\n\nrast_0207 <- st_mosaic(tile1_0207, tile2_0207)\nrast_0216 <- st_mosaic(tile1_0216, tile2_0216)\n\n\n#--- Visualize the rasters that we created \nplot(rast_0207) \n\ndownsample set to 6\n\n\n\n\nplot(rast_0216)\n\ndownsample set to 6\n\n\n\n\n\nHow to change a stars object into an sf object:\n\nsfrast_0207 <- st_as_sf(rast_0207) #sf object in wgs84\n\n\nCreate a Blackout Mask\n\nfind the change in night lights intensity (presumably) caused by the storm\nreclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nassign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\n\n\n#--- Get the difference in values for a new raster diff_rast\ndiff_rast <- rast_0216 - rast_0207 #stars object\ndiff_rast_over200 <- diff_rast > 200\ndiff_rast_over200[diff_rast_over200 == 0] = NA\nplot(diff_rast_over200) #stars object\n\nWarning in plot.stars(diff_rast_over200): breaks=\"quantile\" leads to a single\nclass; maybe try breaks=\"equal\" instead?\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nVectorize the Mask\n\nuse st_as_sf() to vectorize the blackout mask\nfix any invalid geometries using st_make_valid\n\n\n#--- changing stars object to sf object\nblackout_vect <- st_as_sf(diff_rast_over200) |> \n  st_make_valid()\nplot(blackout_vect)\n\n\n\n\n\n\nCrop the Vectorized Map to Our Region of Interest\n\ndefine the Houston metropolitan area with the following coordinates\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nturn these coordinates into a polygon using st_polygon\nconvert the polygon into a simple feature collection using st_sfc() and assign a CRS\n\nbecause we are using this polygon to crop the night lights data it needs the same CRS\n\ncrop (spatially subset) the blackout mask to our region of interest \nre-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n#--- create a list of points for your polygon...THE ORDER MATTERS..\n#--- had to add the first point again at the end \n#--- to avoid an error about it not being 'closed'\nhouston_list = list(rbind(c(-96.5, 29), c(-96.5, 30.5), \n                          c(-94.5, 30.5), c(-94.5, 29), \n                          c(-96.5, 29)))  \n\n#--- now form a polygon from those points \nhouston_polygon <- st_polygon(houston_list) # 'polygon'\n\n#--- convert this to an sf object \nhouston_poly_sfc <- st_sfc(houston_polygon, crs = 4326) # 'sfc polygon'\nhouston_poly_sf <- st_as_sf(houston_poly_sfc) # 'sf object'\n\n#--- make spatial subset of the blackout mask with the polygon\nhouston_subset <- st_crop(blackout_vect, houston_poly_sf) # in wgs84\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n#--- convert this subset to EPSG 3083 projection\nhouston_blackout <- st_transform(houston_subset, \"EPSG: 3083\") # in NAD83\n\n\n\nExclude Highways from Blackout Mask\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\n\n\ndefine SQL query\nload just highway data from geopackage using st_read\nreproject data to EPSG:3083\nidentify areas within 200m of all highways using st_buffer\n\nst_buffer produces undissolved buffers, use st_union to dissolve them\n\nfind areas that experienced blackouts that are further than 200m from a highway\n\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways <- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query)\n\n#--- Define SQL query\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n#--- Load highway data from geopackage\nhighways <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_roads_free_1.gpkg\", query = query)\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'' from data source `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n#--- Set CRS\nhighways3083 <- st_transform(highways, \"EPSG:3083\")\nhighways3083_geom <- highways3083$geom\n\n#--- Create buffer of 200m from highways (I believe that meters is default units for NAD83)**\n#--- highway_buffer is now \"sfc_MULTIPOYGON\"\nhighway_buffer <- st_buffer(highways3083_geom, dist = 200) |> st_union()\n\n#--- Now we can make our mask\nhouston_no_hwy <- houston_blackout[highway_buffer, op = st_disjoint]\n\n\n\nFind homes impacted by blackouts\n\nload buildings data\n\nload buildings dataset using st_read and the following SQL query to select only residential buildings\nreproject data to EPSG:3083\n\n\nhomes_query <- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nhomes <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_buildings_a_free_1.gpkg\", query = homes_query) |> \n  st_transform(\"EPSG:3083\")\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')' from data source `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n\n\n\nfind homes in blackout areas\n\nfilter to homes within blackout areas\ncount number of impacted homes\n\n\n#--- filter for the homes that are within the blackout vector (I'm assuming we are using what I just made above for this)\nblackout_homes <- homes[houston_no_hwy, \"geom\"]\n\n#--- count the number of homes in this subset\nprint(nrow(blackout_homes)) #--- 71,233 homes affected\n\n[1] 71233\n\n\n\n\n\nInvestigate socioeconomic factors\n\nload ACS data\n\nuse st_read() to load the geodatabase layers\ngeometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer\nincome data is stored in the X19_INCOME layer\nselect the median income field B19013e1\nhint: reproject data to EPSG:3083\n\n\n\n#--- read geometries layer and change the column to match \ntract <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", \n                      layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") |> \n  mutate(GEOID = GEOID_Data) \n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\n#--- read in the income layer and select columns GEOID and median income\nmedian_income <- st_read(\"/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                     layer = \"X19_INCOME\", \n                     geometry_column = \"GEOID\") |> \n  select(\"GEOID\", \"B19013e1\") \n\nReading layer `X19_INCOME' from data source \n  `/Users/jaredpetry/Documents/MEDS/fall/eds223_geospatial/assignments/assignment3-jaredbpetry/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\n\n\nDetermine which census tracts experienced blackouts\n\njoin the income data to the census tract geometries\nmake sure to join by geometry ID\nspatially join census tract data with buildings determined to be impacted by blackouts\nfind which census tracts had blackouts\n\n\n\n#--- left join with the blackout info to get geometries\ntract_w_income <- left_join(tract, median_income, \n                           by = \"GEOID\") |> \n  st_transform(crs = \"EPSG:3083\") |> \n  rename(median_income = B19013e1)\n\n#--- spatially join with buildings that experienced blackouts (blackout_homes)\nblack_tract_vector <- st_join(blackout_homes, tract_w_income, \n                            join = st_within)\n# plot(income_buildings$geometry) \n# plot(blackout_homes)\n\n#--- find which census tracts have homes that experienced blackouts\n#--- to do this, find the unique rows by census tract from the object above\nprint(n_distinct(black_tract_vector$TRACTCE))\n\n[1] 629\n\n#--- this says that there are 629 unique tracts with homes that experienced blackouts\n\n#--- Make sure this is right with a couple methods:\nblack_tract_unique <- unique(black_tract_vector$TRACTCE)\n#--- also has 629 tracts with blackouts \n\n# create a subset of tracts that had blackouts in them\nblackout_tracts <- tract[tract$TRACTCE %in% \nblack_tract_unique,]\n\n# create a subset of tracts with no blackouts \nunaffected_tracts <- tract[!(tract$TRACTCE %in% black_tract_unique),]\n\n\n\nCompare incomes of impacted tracts to unimpacted tracts\n\ncreate a map of median income by census tract, designating which tracts had blackouts\n\n\n#--- subset all the cencus tracts to houston only:\n#--- follow the same steps as before: crop while in wgs84 and then transform to \n#--- NAD83 after\n\nhouston_all_tracts <- tract_w_income |> \n  st_transform(\"EPSG:4326\") |> \n  st_crop(houston_poly_sf) |> \n  st_transform(\"EPSG:3083\")  \n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n#--- also crop the blackout_tracts object\n\naffected_tract_cropped <- blackout_tracts |> \n  st_transform(\"EPSG:4326\") |> \n  st_crop(houston_poly_sf) |> \n  st_transform(\"EPSG:3083\")\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n#--- same thing for the unnafected tracts \n\nunaffected_tracts_cropped <- unaffected_tracts |> \n  st_transform(\"EPSG:4326\") |> \n  st_crop(houston_poly_sf) |> \n  st_transform(\"EPSG:3083\")\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n#--- Plot all tracts versum the blackout tracts to make sure we're on track:\n\n#plot all the houston tracts \nhouston_all_tracts_map <- tm_shape(houston_all_tracts) +\n  tm_polygons() \nhouston_all_tracts_map\n\n\n\n#--- Plot just blackout tracts \nblackout_tracts_map <- tm_shape(affected_tract_cropped) +\n  tm_polygons()\nblackout_tracts_map\n\n\n\n#--- Make a plot to see how that went\nunaffected_tracts_map <- tm_shape(unaffected_tracts_cropped) + \n  tm_polygons() \nunaffected_tracts_map"
  },
  {
    "objectID": "Posts/houston_blackout/index.html#results",
    "href": "Posts/houston_blackout/index.html#results",
    "title": "Investigating Socioecomonic Factors in the 2021 Houston Blackout with Geospatial Data",
    "section": "Results",
    "text": "Results\n\nMake the Map\nstart with all the tracts and color based on income\n\n#--- how are we gonna get the blackout status in the map?\n#--- make a dataframe with just tract, geoid, and median income\n#tract_geoid_income\n\n#--- First let's try making a map of just income and census tract\n\nmedian_income_map <- tm_shape(houston_all_tracts) + \n  tm_fill(col = \"median_income\", palette = \"YlOrRd\") +\n  tm_borders() +\n  tm_layout(legend.outside = T, \n            main.title = \"Median Income by Census tract\",\n            frame = T)\nmedian_income_map\n\n\n\n#--- Since there's a lot going on here, let's split into two maps to compare the blackout tracts vs. the anaffected tracts \n\n#--- add income data to affected tracts\nblackout_w_income <- st_join(affected_tract_cropped, tract_w_income)\n\n#--- make map of blackout tracts\nblackout_w_income_map <- tm_shape(blackout_w_income) + \n  tm_fill(col = \"median_income\", palette = \"YlOrRd\") +\n  tm_layout(legend.position = c(\"right\", \"top\"),\n            legend.width = .3,\n            legend.height = .2,\n            main.title = \"Income in affected tracts\",\n            main.title.size = 1,\n            frame = T)\n\n#--- add income data to unaffected tracts\nunaffected_w_income <- st_join(unaffected_tracts_cropped, tract_w_income)\n\n#--- make map of unaffected tracts\nunaffected_w_income_map <- tm_shape(unaffected_w_income) + \n  tm_fill(col = \"median_income\", palette = \"YlOrRd\") +\n  tm_layout(legend.show = F, \n            main.title = \"Income in unaffected tracts\",\n            main.title.size = 1,\n            frame = T)\n\ntmap_arrange(blackout_w_income_map, unaffected_w_income_map, \n             ncol = 2, \n             nrow = 1)\n\n\n\n\nplot the distribution of income in impacted and unimpacted tracts\n\n#--- make two boxplots next to eachother with income data of tracts to compare blackout vs. non-blackout tracts \n\nboxplot(blackout_w_income$median_income, unaffected_w_income$median_income, \n        names = c(\"affected\", \"unaffected\"))\n\n\n\n\n\nDiscussion:\nUpon looking at the maps and boxplot I made above, there does not seem to be a very significant difference in the median income between the two groups: those affected by the blackout and those not affected. That really surprised me because I believe that the electrical infrastructure in lower income neighborhoods would have likely been older and more subject to failure while the grid was in a stressful state during this weather event. I would say that a limitation to this study is the fact that the two satellite images that we started with happened to be a bit far apart temporally. I would hypothesize that if we were able to obtain a photo right before the blackout happened, and then right when the blackout was at its most severe, we would have been able to see a larger difference, and possibly seen more significant results. Additionally, the data may have been slightly misleading for the income data because the areas farther from the city seemed to be much less affected, and likely contain very different demographics than those closer to the urban areas of Houston. I noticed that the more rural areas tended to have slightly lower median incomes. It would be interesting to focus on a smaller area, because these large tracts may have skewed our data a bit."
  },
  {
    "objectID": "Posts/eel_model/index.html",
    "href": "Posts/eel_model/index.html",
    "title": "Tuning an XGBoost Machine Learning Model to Predict Eel Presence",
    "section": "",
    "text": "This blog post loosely follows the boosted regression tree exercise outlined in the academic paper: “A working guide to boosted regression trees” by Edith et al. We will use the same dataset that they did on the distribution of the short finned eel (Anguilla australis). We will be using the xgboost library, tidymodels, caret, parsnip, vip, and more.\nCitation:\nElith, J., Leathwick, J. R., & Hastie, T. (2008). A working guide to boosted regression trees. Journal of Animal Ecology, 77(4), 802–813. https://doi.org/10.1111/j.1365-2656.2008.01390.x\n\nlibrary(tidyverse) \n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.0     ✔ rsample      1.1.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.3     ✔ workflows    1.0.0\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.1     ✔ yardstick    1.1.0\n✔ recipes      1.0.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(gbm)\n\nLoaded gbm 2.1.8.1\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(ggpubr)\nlibrary(tictoc)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nRead in the data\n\neel_dat <- read_csv(\"eel.model.data.csv\") |> \n  mutate(Angaus = as.factor(Angaus))\n\nRows: 1000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Method\ndbl (13): Site, Angaus, SegSumT, SegTSeas, SegLowFlow, DSDist, DSMaxSlope, U...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nSplit the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus\n\n# create initial split\neel_split <- initial_split(data = eel_dat, \n                           prop = 0.7, \n                           strata = Angaus)\n\n# create training and testing data from the split \neel_train <- training(eel_split) \neel_test <- testing(eel_split)\n\n# resample the training set \neel_folds <- vfold_cv(data = eel_train, \n                      v = 10,\n                      strata = Angaus)\n\n\n\n\nCreate a recipe to prepare the data for the XGBoost model. We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis\n\neel_recipe <- recipe(Angaus ~ ., data = eel_train) |> \n  step_integer(all_predictors(), zero_based = TRUE)"
  },
  {
    "objectID": "Posts/eel_model/index.html#tuning-xgboost",
    "href": "Posts/eel_model/index.html#tuning-xgboost",
    "title": "Tuning an XGBoost Machine Learning Model to Predict Eel Presence",
    "section": "Tuning XGBoost",
    "text": "Tuning XGBoost\nWe are going to tune 3 different times to get the ideal hyperparamters in our model. We first tune the learning rate and get the estimation of the best learning rate. Then we take that learning rate and set it as fixed in the next tuning. Next, we tune the three tree paramters: tree_depth, loss_reduction, and min_n. Lastly, we will set those as fixed for the stochastic tuning (m_try).\n\nTune Learning Rate\n\nCreate a model specification using {xgboost} for the estimation\n\n\nOnly specify learning rate parameter to tune()\n\n\n# create model specification: (tune learning rate first)\neel_boost_model <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = NULL,\n  loss_reduction = NULL, \n  learn_rate = tune(),\n  min_n = NULL,\n  mtry = NULL\n  )\n\n\nSet up a grid to tune the model by using a range of learning rate parameter values.\n\n\nlearn_rate_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\nComputational efficiency becomes a factor as models get more complex and data gets larger. Becuase of this, we will be recording the time it takes to run with Sys.time().\nTune the learning rate parameter:\n\n# tune the learning rate:\n\n# define a workflow for tuning the learning rate: \neel_learn_wf <- workflow() |> \n  add_model(eel_boost_model) |> \n  add_recipe(eel_recipe)\n\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_learn <- eel_learn_wf |> \n  tune_grid(\n    eel_folds, \n    grid = learn_rate_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n\n[1] \"time elapsed: 2.00375526779228\"\n\n\n\nShow the performance of the best models and the estimates for the learning rate parameter values associated with each.\n\n\nshow_best(tune_learn, metric = \"roc_auc\")  # using metric roc_auc \n\n# A tibble: 5 × 7\n  learn_rate .metric .estimator  mean     n std_err .config              \n       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     0.290  roc_auc binary     0.814    10  0.0164 Preprocessor1_Model29\n2     0.207  roc_auc binary     0.813    10  0.0169 Preprocessor1_Model21\n3     0.0311 roc_auc binary     0.812    10  0.0162 Preprocessor1_Model04\n4     0.186  roc_auc binary     0.811    10  0.0192 Preprocessor1_Model19\n5     0.238  roc_auc binary     0.810    10  0.0178 Preprocessor1_Model24\n\n# --- learn rate = 0.04146552... use this because the paper did\nshow_best(tune_learn, metric = \"accuracy\") \n\n# A tibble: 5 × 7\n  learn_rate .metric  .estimator  mean     n std_err .config              \n       <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1     0.0208 accuracy binary     0.831    10 0.00859 Preprocessor1_Model03\n2     0.0104 accuracy binary     0.831    10 0.00695 Preprocessor1_Model02\n3     0.0311 accuracy binary     0.830    10 0.00850 Preprocessor1_Model04\n4     0.104  accuracy binary     0.827    10 0.00875 Preprocessor1_Model11\n5     0.0725 accuracy binary     0.827    10 0.00978 Preprocessor1_Model08\n\n\n\n\nTune Tree Parameters\n\nCreate a new specification where you set the learning rate (which we already optimized) and tune the tree parameters.\n\n\n# create model specification for tuning tree depth with new (optimized) learning rate\neel_boost_model2 <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = tune(),\n  loss_reduction = tune(), \n  learn_rate = 0.04146552,\n  min_n = tune(),\n  mtry = NULL\n  )\n\n# define workflow for tuning tree parameters: \ntree_tune_wf <- workflow() |> \n  add_model(eel_boost_model2) |> \n  add_recipe(eel_recipe)\n\n\nSet up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space\n\n\n# we are now tuning the tree parameters: tree_depth(), min_n(), and loss_reduction()\n\n# use tidymodels dials package to specify which paramters we are trying to tune... \n# --- grid_max_entropy() needs an object like this to work properly\n\ntree_depth_param <- dials::parameters(\n  tree_depth(), \n  min_n(), \n  loss_reduction()\n)\ntree_tune_grid <- grid_max_entropy(tree_depth_param, size = 60)\n\n\nShow the performance of the best models and the estimates for the tree parameter values associated with each.\n\n\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_tree <- tree_tune_wf |> \n  tune_grid(\n    eel_folds, \n    grid = tree_tune_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n\n[1] \"time elapsed: 9.25062733491262\"\n\nshow_best(tune_tree, metric = \"roc_auc\")\n\n# A tibble: 5 × 9\n  min_n tree_depth loss_reduction .metric .estimator  mean     n std_err .config\n  <int>      <int>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1     8          3       5.80e+ 0 roc_auc binary     0.832    10  0.0105 Prepro…\n2     5         10       1.03e+ 1 roc_auc binary     0.830    10  0.0101 Prepro…\n3    13         15       2.26e+ 0 roc_auc binary     0.828    10  0.0107 Prepro…\n4    15          1       3.50e-10 roc_auc binary     0.824    10  0.0162 Prepro…\n5    18         10       3.52e-10 roc_auc binary     0.822    10  0.0154 Prepro…\n\n# best value for min_n: 17\n# best value for tree_depth: 11\n# best value for loss_reduction: 0.367\n\n\n\nTune Stochastic Parameters\n\nWe will create a new specification where we set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters (m_try).\n\n\n# model specification with fixed learning rate and tree parameters to tune stochastic params\neel_boost_model3 <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = 11,\n  loss_reduction = 0.367, \n  learn_rate = 0.04146552,\n  min_n = 17,\n  mtry = tune(), \n  sample_size = tune()\n  )\n\n# define workflow for tuning stochastic parameters: \nstoch_tune_wf <- workflow() |> \n  add_model(eel_boost_model3) |> \n  add_recipe(eel_recipe)\n\n\nSet up a tuning grid using grid_max_entropy() again.\n\n\nstoch_param <- dials::parameters(\n  finalize(mtry(), select(eel_train, -Angaus)), \n  sample_size = sample_prop(c(0.4, 0.9))\n)\nstoch_grid <- grid_max_entropy(stoch_param, size = 60)\n\n\nShow the performance of the best models and the estimates for the tree parameter values associated with each.\n\n\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_stoch <- stoch_tune_wf |> \n  tune_grid(\n    eel_folds, \n    grid = stoch_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n\n[1] \"time elapsed: 5.46083606077565\"\n\nshow_best(tune_stoch, metric = \"roc_auc\")\n\n# A tibble: 5 × 8\n   mtry sample_size .metric .estimator  mean     n std_err .config              \n  <int>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     5       0.887 roc_auc binary     0.827    10  0.0160 Preprocessor1_Model05\n2     9       0.898 roc_auc binary     0.825    10  0.0165 Preprocessor1_Model27\n3     6       0.882 roc_auc binary     0.824    10  0.0163 Preprocessor1_Model57\n4     8       0.899 roc_auc binary     0.824    10  0.0169 Preprocessor1_Model04\n5     5       0.827 roc_auc binary     0.824    10  0.0160 Preprocessor1_Model54\n\n# best value for mtry: 1\n# best value for sample size: 0.8932816"
  },
  {
    "objectID": "Posts/eel_model/index.html#finalize-workflow-and-make-final-prediction",
    "href": "Posts/eel_model/index.html#finalize-workflow-and-make-final-prediction",
    "title": "Tuning an XGBoost Machine Learning Model to Predict Eel Presence",
    "section": "Finalize workflow and make final prediction",
    "text": "Finalize workflow and make final prediction\n\nAssemble your final workflow with all of your optimized parameters and do a final fit.\n\n\n# model specification with optimal parameters\neel_boost_final <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = 11,\n  loss_reduction = 0.367, \n  learn_rate = 0.04146552,\n  min_n = 17,\n  mtry = 1, \n  sample_size = 0.8932816\n  )\n\n# define workflow for final fit\nstoch_tune_wf <- workflow() |> \n  add_model(eel_boost_final) |> \n  add_recipe(eel_recipe)\n\nfinal_eel_fit <- last_fit(eel_boost_final, \n                          Angaus ~ ., \n                          eel_split)\n\nfinal_eel_fit$.predictions\n\n[[1]]\n# A tibble: 301 × 6\n   .pred_0 .pred_1  .row .pred_class Angaus .config             \n     <dbl>   <dbl> <int> <fct>       <fct>  <chr>               \n 1  0.0914  0.909      2 1           1      Preprocessor1_Model1\n 2  0.660   0.340      3 0           0      Preprocessor1_Model1\n 3  0.974   0.0261     7 0           0      Preprocessor1_Model1\n 4  0.873   0.127      8 0           0      Preprocessor1_Model1\n 5  0.796   0.204      9 0           0      Preprocessor1_Model1\n 6  0.949   0.0510    11 0           0      Preprocessor1_Model1\n 7  0.859   0.141     20 0           0      Preprocessor1_Model1\n 8  0.915   0.0850    21 0           0      Preprocessor1_Model1\n 9  0.986   0.0136    28 0           0      Preprocessor1_Model1\n10  0.709   0.291     36 0           0      Preprocessor1_Model1\n# … with 291 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\nboost_tree_metrics <- final_eel_fit |> collect_metrics()\nboost_tree_accuracy <- boost_tree_metrics$.estimate[1]\nprint(paste0(\"the decision tree model accuracy came out to: \", boost_tree_accuracy))\n\n[1] \"the decision tree model accuracy came out to: 0.820598006644518\"\n\n\n\nHow well did your model perform? What types of errors did it make?\n\nmake a confusion matrix\n\n# to make a confusion matrix we need a table of the predictions vs the true values\nboost_predictions <- final_eel_fit$.predictions[[1]]\n\n# make a simple table of just the predictions and actual values\nconfusion_table <- boost_predictions |> \n  select(c(.pred_class, Angaus))\n\n# create a confusion matrix comparing the predictions with actual observations\nconfusionMatrix(data = confusion_table$.pred_class, \n                reference = confusion_table$Angaus)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 215  29\n         1  25  32\n                                          \n               Accuracy : 0.8206          \n                 95% CI : (0.7725, 0.8623)\n    No Information Rate : 0.7973          \n    P-Value [Acc > NIR] : 0.1761          \n                                          \n                  Kappa : 0.431           \n                                          \n Mcnemar's Test P-Value : 0.6831          \n                                          \n            Sensitivity : 0.8958          \n            Specificity : 0.5246          \n         Pos Pred Value : 0.8811          \n         Neg Pred Value : 0.5614          \n             Prevalence : 0.7973          \n         Detection Rate : 0.7143          \n   Detection Prevalence : 0.8106          \n      Balanced Accuracy : 0.7102          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nWell, looks like my model did OK. It got an accuracy of 0.8339 which is not too bad… from looking at the confusion matrix, most of the errors that were made were false negative predictions, which were over twice the number of false positive predictions."
  },
  {
    "objectID": "Posts/eel_model/index.html#fit-your-model-the-evaluation-data-and-compare-performance",
    "href": "Posts/eel_model/index.html#fit-your-model-the-evaluation-data-and-compare-performance",
    "title": "Tuning an XGBoost Machine Learning Model to Predict Eel Presence",
    "section": "Fit your model the evaluation data and compare performance",
    "text": "Fit your model the evaluation data and compare performance\n\nNow we will fit the final model to the big dataset used in the paper.\n\n\n# read in the eval data \neval_data <- read_csv(\"eel.eval.data.csv\") |> \n  mutate(Angaus_obs = as.factor(Angaus_obs))\n\nRows: 500 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Method\ndbl (12): Angaus_obs, SegSumT, SegTSeas, SegLowFlow, DSDist, DSMaxSlope, USA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# finalize our model: \nbest_params <- select_best(tune_stoch)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\nfinal_model <- finalize_model(eel_boost_final, parameters = best_params)\n\n# make predictions with our model\neval_fit <- final_model |> fit(Angaus_obs ~ ., data = eval_data)\n\n\nHow does the model perform on this data?\n\n\n# generate the predicted outcomes \neval_preds <- eval_fit |> predict(new_data = eval_data)\neval_pred_probs <- eval_fit |> predict_classprob.model_fit(new_data = eval_data)\njoined_predictions <- bind_cols(eval_data, eval_preds, eval_pred_probs)\n\n# assess model performance with a confusion matrix \nconfusionMatrix(data = joined_predictions$.pred_class, \n                reference = joined_predictions$Angaus_obs)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 370  57\n         1  23  50\n                                         \n               Accuracy : 0.84           \n                 95% CI : (0.8049, 0.871)\n    No Information Rate : 0.786          \n    P-Value [Acc > NIR] : 0.0014538      \n                                         \n                  Kappa : 0.4622         \n                                         \n Mcnemar's Test P-Value : 0.0002247      \n                                         \n            Sensitivity : 0.9415         \n            Specificity : 0.4673         \n         Pos Pred Value : 0.8665         \n         Neg Pred Value : 0.6849         \n             Prevalence : 0.7860         \n         Detection Rate : 0.7400         \n   Detection Prevalence : 0.8540         \n      Balanced Accuracy : 0.7044         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nWoohoo! we got an even better accuracy than with the testing data that we fit the model to! Accuracy was 84.4% Still over twice as many false negative errors than false positive ones\n\nHow do our results compare to those of Elith et al.?\n\n\nAs for variable importance, I got just about the same results for the most important predictors being: summer air temp, distance to coast, native vegetation, downstream max slope… execpt, in the paper one of the most important was the fishing method… which didn’t pop up for me which I thought was interesting. The roc area under the curve that the scientists from the paper acheived was 0.858, which was slightly higher than mine… dang! But at least it was really close."
  },
  {
    "objectID": "Posts/eel_model/index.html#variable-importance",
    "href": "Posts/eel_model/index.html#variable-importance",
    "title": "Tuning an XGBoost Machine Learning Model to Predict Eel Presence",
    "section": "Variable Importance",
    "text": "Variable Importance\nUsing the package {vip} to compare variable importance. This is really cool because we can see which variables influenced the model the most once it is finalized.\n\n# create a plot of variable importance\nvip(eval_fit)"
  },
  {
    "objectID": "Posts/eel_model/index.html#discussion",
    "href": "Posts/eel_model/index.html#discussion",
    "title": "Tuning an XGBoost Machine Learning Model to Predict Eel Presence",
    "section": "Discussion",
    "text": "Discussion\nWhat do our variable importance results tell us about the distribution of this eel species?\n\nSummer air temperature is very important to this species because it was the most influential variable in the model. Maybe some part of their breeding or other important stage of their life cycle occurs in summer. From the code I wrote below, it seems like they like warmer temperatures on average in the summer.\n\n\nTheir distance to the coast is also very important in determining the prescence of this eel species. The average distance for the dataset while the eel is present is HALF of that with the eel absent, so I’m thinking that the eel likes to be closer to the coast rather than farther. Maybe this is because they are anadromous in some way or need brackish water for part of their life cycle.\n\n\nThe species also seems to be heavily influence in the amount of native forest that the particular habitat contains… no surprise there!\n\n\nThe species also seems to like areas with more gently sloped downstream areas, which to me suggests that they somewhat rely on being able to travel up and downstream…\n\n\n# make a little subset dataframe for just the places that the eel was present\neel_pres <- eel_dat |> \n  filter(Angaus == 1)\neel_abs <- eel_dat |> \n  filter(Angaus == 0)\n\n# find out what summer temperature they like \nmean(eel_pres$SegSumT) # = 17.8005\n\n[1] 17.8005\n\nmean(eel_abs$SegSumT) # = 16.8005\n\n[1] 16.07343\n\n# find out if they like to be closer or farther from the coast\nmean(eel_pres$DSDist) # = 42.47604\n\n[1] 42.47604\n\nmean(eel_abs$DSDist) # = 82.60588\n\n[1] 82.60588\n\n# find out whether they like steeper slope or shallower slope \nmean(eel_pres$DSMaxSlope) # = 1.544\n\n[1] 1.544554\n\nmean(eel_abs$DSMaxSlope) # = 3.395\n\n[1] 3.395376"
  }
]