[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jared Petry",
    "section": "",
    "text": "Education\n\n\nLaptop Code Masters of Environmental Data Science, 2023\n\n\n\nBren School of Environmental Science and Management, University of California, Santa Barbara\n\n\n\nMicroscope BS in Environmental Studies, 2021\n\n\n\nUniversity of California Santa Barbara\n\n\n\n\n\nExperience\n\n\nBar Chart Fisheries Technician, 2022\n\n\n\nPacific States Marine Fisheries Commission\n\n\n\nFish Sustainability Coordinator and Seafood Retailer, 2021\n\n\n\nKanaloa Seafood, Santa Barbara\n\n\n\n\nMagnifying glass Biodiversity Analysis Intern, 2021\n\n\n\nMarine Biodiversity Observation Network\n\n\n\nTree Restoration Intern, 2019-2021\n\n\n\nCheadle Center for Biodiversity and Ecological Restoration"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Jared is a student of the Masters of Environmental Data Science program at UC Santa Barbara. He obtained his environmental studies bachelors of science degree in 2021 at UC Santa Barbara as well, with a focus on ecology. After graduating, Jared spent a year working in both sustainable seafood and salmonid restoration. Jared is interested in applying his growing data science knowledge to creating models that imitate natural systems in order to understand our rapidly changing world. Specifically, he is interested in marine habitat restoration, fisheries and aquaculture science, environmental justice, and the effects of coastal development on marine ecosystems and recreational activities. While believing that our future depends on the health of our oceans, Jared is optimistic that responsible ocean aquaculture can sustain our growing demand for healthy seafood protein. Jared can usually be found surfing and exploring novelty, remote beaches around Santa Barbara County. In the future, he hopes to learn more about the impact of climate change on recurring weather patterns, longshore sand flow, and coastal systems in general."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "“Determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of marine organims”\n\n\n\n\n\n\nDec 1, 2022\n\n\nJared Petry\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n“Can we use buoy data in R to determine whether or not there are long run trends in wave height, period, and direction? What role does the El Niño Southern Oscillation (ENSO) have in determining patterns in these variables?”\n\n\n\n\n\n\nDec 1, 2022\n\n\nJared Petry\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Posts/stats_buoy_project/index.html",
    "href": "Posts/stats_buoy_project/index.html",
    "title": "Statistical Time-series Analysis of Data from Harvest Buoy",
    "section": "",
    "text": "Background\nLiving in Santa Barbara can be tricky from a surfer’s perspective.  When I moved here from San Diego, I learned that it takes far more diligence and knowledge of reading meteorological data in order to score some fun waves here.  If you really know what you’re doing, you can even get some great spots all to yourself some days. \nLately, I have been hearing from the older locals that we have had the worst three winters in a row that they can remember. As I learned more about climate change in my undergraduate career alongside my own surfing research, I began to wonder if there’s a connection between climate change and surfing opportunities.  Is this all in our heads as frustrated SB surfers? Could the increasing variability of storms due to climate change affect the surf in Santa Barbara?\nThis short blog post will focus on wave prevalence and attributes over time due to patterns in storm activity.  I would like to use R to find out whether there has been a trend or pattern in surf availability based on time series buoy data using classical decomposition. If possible, I would also like to link this to the ENSO cycles.\n\n\n\nMotivation\nMost, if not all adventure sports are being somewhat impacted by climate change due to their dependency on wind, rainfall, temperature, swell, snowmelt, and other climate variables.  We have already seen effects of increased climate variability on the adventure tourism industry, specifically in snow sports (Buckley, 2017).  But how does climate change relate to surfing? Rising sea level paired with anthropogenic variables such as coastal development and depleted sand banks (from upstream hydroelectric dams) combine to exacerbate coastal erosion. This can alter underwater bathymetry that creates the waves we salty people depend on (Arroyo, 2019).  Many surfing locations all over California have been both tragically lost and accidentally created at the hand of human activity.  \nWithin the realm of surf frequency and quality, a key predictor of any given year’s surfing potential in California is the El Niño Southern Oscillation (ENSO) status.  During El Niño years, the storm track over the North Pacific is usually enhanced, and we see swells travel from straight West through the Santa Barbara Channel. During strong events, California sees especially significant surf.  During La Niña, blocking high pressure over the pacific during winter typically limits tropical swells in the eastern pacific, and shifts the storm track to the North, causing many swells to miss the Santa Barbara Channel (Surfline, 2020).  \nThis winter (2022-23), there is a 76 percent chance that we will be experiencing our third La Niña even in a row, which is relatively rare (Johnson, 2022).  Climate change is causing some strange anomalies in the ENSO cycle.  According to National Oceanic and Atmospheric Association (NOAA) researcher Michael McPhaden, “Extreme El Niño and La Niña events may increase in frequency from about one every 20 years to one every 10 years by the end of the 21st century under aggressive greenhouse gas emission scenarios.” (research.noaa.gov).  The last extreme ENSO event that we have had was the winter of 2015-2016, when a brutal stream of storms battered the Santa Barbara coastline, causing considerable damage, and amazing surf.\n\n\nAbout the data\n\n\nShow Code\nknitr::include_graphics(\"images/buoy_pic.png\")\n\n\n\n\n\nI used data from the buoy that I look at every day that guides all my surf decision making, the Harvest Buoy off Point Conception.  It is owned by the Scripps Institute of Oceanography and its data is distributed by NOAA.  This buoy has been up and running since 2004, but the data has a few large gaps (I think because of maintenance or malfunction reasons) up until 2010.  From 2010 onward, there are two observations every single hour of every day until this moment.  The measurements that I am interested in are wave height, dominant wave period - basically wavelength, average wave period, and swell direction.  \nThere are seemingly many ways to retrieve this data, but the one that I was able to get to work was through this link:\nhttps://www.ndbc.noaa.gov/station_history.php?station=46218\nYou can access a description of this data and variables through this link: https://www.ndbc.noaa.gov/measdes.shtml\nThe published data is in text files, with fields separated by spaces. I used the base R read.table() function to read the measurements into a data frame.  With this function I still had to do some renaming and matching of the columns but it left the data intact.\nI ended up with this dataframe below which has observations of wave height, dominant period, average period, peak swell direction, and temperature. The variables that I felt were relevant to this project were wave height (feet), swell direction (degrees), and dominant period (seconds).\n\n\nShow Code\nlibrary(dplyr) \nlibrary(janitor) \nlibrary(here) \nlibrary(tidyverse) \nlibrary(feasts)\nlibrary(readr)\nlibrary(tufte)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(xts)\nlibrary(tsibble)\n\ndf_2010 <- read.table(\"data/txt_files/46218h2010.txt\")\ndf_2011 <- read.table(\"data/txt_files/46218h2011.txt\")\ndf_2012 <- read.table(\"data/txt_files/46218h2012.txt\")\ndf_2013 <- read.table(\"data/txt_files/46218h2013.txt\")\ndf_2014 <- read.table(\"data/txt_files/46218h2014.txt\")\ndf_2015 <- read.table(\"data/txt_files/46218h2015.txt\")\ndf_2016 <- read.table(\"data/txt_files/46218h2016.txt\")\ndf_2017 <- read.table(\"data/txt_files/46218h2017.txt\")\ndf_2018 <- read.table(\"data/txt_files/46218h2018.txt\")\ndf_2019 <- read.table(\"data/txt_files/46218h2019.txt\")\ndf_2020 <- read.table(\"data/txt_files/46218h2020.txt\")\ndf_2021 <- read.table(\"data/txt_files/46218h2021.txt\")\n\nall_wave_df <- rbind(df_2010, df_2011, df_2012, df_2013, df_2014,\n                     df_2015, df_2016, df_2017, df_2018, df_2019, \n                     df_2020, df_2021)\n\nwaves_renamed <- all_wave_df |> \n  rename(year = V1, \n         month = V2, \n         day = V3, \n         hour = V4, \n         minute = V5, \n         wave_height = V9, \n         dom_period = V10,\n         av_period = V11, \n         peak_direction = V12, \n         temp = V15) \n\nwaves_clean <- waves_renamed |> \n  select(year, month, day, hour, minute, wave_height, dom_period,\n         av_period, peak_direction, temp)\nhead(waves_clean) \n\n\n  year month day hour minute wave_height dom_period av_period peak_direction\n1 2010     1   1    0     13        1.65      12.50      8.69            287\n2 2010     1   1    0     43        1.58      12.50      8.67            280\n3 2010     1   1    1     13        1.55      12.50      8.93            290\n4 2010     1   1    1     43        1.62      11.76      9.04            283\n5 2010     1   1    2     13        1.57      12.50      8.63            300\n6 2010     1   1    2     43        1.49      13.33      8.36            273\n  temp\n1 14.2\n2 14.2\n3 14.1\n4 14.1\n5 14.2\n6 14.1\n\n\nTo create a quick visualization of what these variables look like, I calculated the mean of each one per year and made these very simple plots:\n\n\nShow Code\n#--- compare yearly mean wave height \nyearly_mean_wvht <- waves_clean |> \n  group_by(year) |> \n  summarize(wave_height = mean(wave_height)) \n#plot(yearly_mean_wvht)  \n\n#--- compare yearly mean dominant period\nyearly_mean_dom_period <- waves_clean |> \n  group_by(year) |> \n  summarize(period = mean(dom_period))\n#plot(yearly_mean_dom_period) \n\n#--- compare yearly mean swell direction\nyearly_mean_direction <- waves_clean |> \n  group_by(year) |> \n  summarize(direction = mean(peak_direction))\n#plot(yearly_mean_direction)  \n\n#--- Plot the three variables in a 1x3 row\nplot(yearly_mean_wvht, \n     main = \"Average wave height by year\") \n\n\n\n\n\nShow Code\nplot(yearly_mean_dom_period, \n     main = \"Average period by year\") \n\n\n\n\n\nShow Code\nplot(yearly_mean_direction, \n     main = \"Average swell direction by year\") \n\n\n\n\n\nIf you add a linear regression line to the wave height variable across years, you get this slightly concerning downward trend line. What could this mean? We know there are many factors playing a role in the distribution of wave height year to year, specifically seasonality and the El Niño Southern Oscillation cycles. To explore this relationship further, I would like to isolate seasonality and extract a long-run trend from this time series data using classical additive decomposition.\n\n\nShow Code\n#--- add an lm trend line to our wave height averages plot using ggplot\nggplot(data = yearly_mean_wvht, aes(x = year, y = wave_height)) + \n  geom_point(col = \"blue\") + \n  geom_smooth(method = lm, col = \"red\") +\n  labs(title = \"Average wave height by year with linear regression\")\n\n\n\n\n\n\n\nDecomposition\nThe North Pacific comes to life in wintertime, and goes back to sleep in the summer… for the most part. When dealing with very seasonal data like this it can be useful to create a classical additive decomposition model. This model will decompose the data into seasonal, random, and trend components like this: \\[y_t = S_t + T_t + R_t\\]\nAfter we have separated these components we can plot them to compare which significantly impact our data. The classical decomposition model library function requires fixed and exact periodic intervals between data measurements.  The original wave data was taken approximately every 30 minutes and was somewhat regular, but not exact.  So, I converted the measurements to a daily median (more resistant to outliers).  There were still some gaps in the data, so I used tsibble::fill_gaps() to populate the gaps with “na” and tidyr::fill() to convert the “na” values to the last good measurement. After feeding this through our decomposition model, you can see plots generated below for our variables of interest.\n\n\nShow Code\n#--- Add in ISOdate column to use in tsibble index\nwaves_clean$dt <- ISOdate(\n    waves_clean$year,\n    waves_clean$month,\n    waves_clean$day,\n    waves_clean$hour,\n    waves_clean$minute,\n    0\n) |> na.omit()\n\n#--- Convert to time series tibble\nwave_ts = as_tsibble(waves_clean, index=\"dt\") \n\n#--- Create a regular fixed-interval time series by aggregating daily wave\n# height as the mean height for each day.  Also, some days were missing,\n# could be due to wrong days in a month, like Feb, so using fill_gaps() to\n# add any needed rows and fill() to use prior measurement for the gaps.\n\nwave_regular = wave_ts %>%\n  index_by(index = ~ as_date(.)) %>% \n  summarise(\n    wave_height = median(wave_height),\n    dom_period = median(dom_period),\n    av_period = median(av_period),\n    peak_direction = median(peak_direction), \n    temp = median(temp)\n  ) %>%\n  fill_gaps() %>%\n  tidyr::fill(wave_height, .direction = \"down\") %>%\n  tidyr::fill(dom_period, .direction = \"down\") %>%\n  tidyr::fill(av_period, .direction = \"down\") %>%\n  tidyr::fill(peak_direction, .direction = \"down\") %>%\n  tidyr::fill(temp, .direction = \"down\")\n\n# Now, we have a \"regular\" fixed-interval tsibble with median measurements per day, with column 'index'\n\n#--- Classical addititve decomposition of wave height\nwave_regular %>% \n  model(classical_decomposition(wave_height ~ season(\"1 year\"), \n                                type = \"additive\")) %>% \n  components() %>%\n  autoplot() +\n  labs(title = \"Classical addititve decomposition of wave height\")\n\n\n\n\n\nShow Code\n#--- Classical addititve decomposition of dominant wave period\nwave_regular %>% \n  model(classical_decomposition(dom_period ~ season(\"1 year\"), \n                                type='additive')) %>%\n  components() %>%\n  autoplot() + labs(title = \"Classical addititve decomposition of dominant period\")\n\n\n\n\n\nShow Code\n#--- Classical addititve decomposition of average wave period\nwave_regular %>% \n  model(classical_decomposition(av_period ~ season(\"1 year\"), \n                                type='additive')) %>%\n  components() %>%\n  autoplot() + labs(title = \"Classical addititve decomposition of average period\")\n\n\n\n\n\nShow Code\n#--- Classical addititve decomposition of peak swell direction\nwave_regular %>% \n  model(classical_decomposition(peak_direction ~ season(\"1 year\"), \n                                type='additive')) %>%\n  components() %>%\n  autoplot() + labs(title = \"Classical addititve decomposition of peak swell direction\")\n\n\n\n\n\n\n\nDecomposition Analysis\nAs you can tell looking at these decomposition plots, there’s no way to make out a definitive trend in any of the variables, whether they are going up or down over time. Additionally, this is a very short time period in the grand scheme of climate change over time since the industrial revolution. Perhaps we could visualize more meaningful relationships over time if these measurements had taken place since the 1940s or so.\nHowever, we do see an interesting spike in wave height and period during a time period around 2016. We also see a sharp drop in swell direction to about an average of 280 degrees. These conditions are all very favorable to surfing in Santa Barbara. This time period around 2016 also aligns with the very strong El Niño event that took place that year. Here is a useful graphic of El Niño oscillations over our time period of interest (Null, 2022).\n\n\n\n\n\n\n\nAnother Question\nSince we see the variables changing during this El Niño event, I would like to compare our wave height, period, and direction during a strong El Niño event with that of a strong La Niña event. Since there is only one of each happening within the 2010-2021 period that we have buoy data for, I will compare the 2010-11 La Niña event (June 1, 2010 - May 31, 2011) with the 2015-16 El Niño event (April 1, 2015 - March 31, 2016).\nFirst, I’ll subset our time series dataframe into just those time periods, with an added enso column that will either say “el_nino” or “la_nina”. Then I will be able to use the stats::t.test() function to determine whether the difference in means of our variables of interest differ statistically between strong El Niño and La Niña events.\n\n\nShow Code\n#--- create la nina filter: june 1, 2010- may 31 2011 and create an enso column that says that it was la nina\n\nla_nina <- wave_regular |> \n  filter(between(index, as.Date(\"2010-06-01\"), as.Date(\"2011-05-31\"))) \nla_nina$enso <- \"la_nina\"\n  \n#--- create el nino filter: june 1, 2010- may 31 2011 and create an enso column that says that it was el nino\n\nel_nino <- wave_regular |> \n  filter(between(index, as.Date(\"2015-04-01\"), as.Date(\"2016-03-31\"))) \nel_nino$enso <- \"el_nino\"\n\n#--- visualize the means of our variables \ncomparison_chart <- data.frame(enso = c(\" 2010-11 la nina\", \"2015-16 el nino\"), \n                               mean_height = c(mean(la_nina$wave_height), \n                                               mean(el_nino$wave_height)),\n                               mean_period = c(mean(la_nina$dom_period), \n                                               mean(el_nino$dom_period)),\n                               mean_direction = c(mean(la_nina$peak_direction),\n                                                  mean(el_nino$peak_direction)))\n                               \ncomparison_chart\n\n\n              enso mean_height mean_period mean_direction\n1  2010-11 la nina    2.203219    11.87426       287.4562\n2  2015-16 el nino    2.309850    12.49772       281.9139\n\n\n\nSignificance Tests\nIn order to test whether these two ENSO events are significantly different in wave height, period, and swell direction, I will run three separate t-tests between the two time periods.\nWave height-\nNull Hypothesis: There is no difference in the mean wave height of the 2010-11 La Niña event and the 2015-16 El Niño event.\nAlternative Hypothesis: There is a difference in the mean wave height of the 2010-11 La Niña event and the 2015-16 El Niño event.\n\n\nShow Code\n#--- join these dataframes together to run t-test\n\ncomparison_df <- bind_rows(la_nina, el_nino)\n\n#--- run t-test to see if enso status really affects our variables\n\n#--- WAVE HEIGHT\nt.test(wave_height~enso, data = comparison_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  wave_height by enso\nt = 1.6275, df = 673.93, p-value = 0.1041\nalternative hypothesis: true difference in means between group el_nino and group la_nina is not equal to 0\n95 percent confidence interval:\n -0.02201407  0.23527517\nsample estimates:\nmean in group el_nino mean in group la_nina \n             2.309850              2.203219 \n\n\nWe fail to reject the null hypothesis at the alpha = 0.05 significance level.\nDominant Period-\nNull Hypothesis: There is no difference in the mean dominant period of the 2010-11 La Niña event and the 2015-16 El Niño event.\nAlternative Hypothesis: There is a difference in the mean dominant period of the 2010-11 La Niña event and the 2015-16 El Niño event.\n\n\nShow Code\n#--- PERIOD\nt.test(dom_period~enso, data = comparison_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  dom_period by enso\nt = 3.0192, df = 720.98, p-value = 0.002624\nalternative hypothesis: true difference in means between group el_nino and group la_nina is not equal to 0\n95 percent confidence interval:\n 0.2180466 1.0288700\nsample estimates:\nmean in group el_nino mean in group la_nina \n             12.49772              11.87426 \n\n\nWe reject the null hypothesis at the alpha = 0.05 significance level.\nPeak Swell Direction-\nNull Hypothesis: There is no difference in the mean peak swell direction of the 2010-11 La Niña event and the 2015-16 El Niño event.\nAlternative Hypothesis: There is a difference in the mean peak swell direction of the 2010-11 La Niña event and the 2015-16 El Niño event.\n\n\nShow Code\n#--- DIRECTION\nt.test(peak_direction~enso, data = comparison_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  peak_direction by enso\nt = -1.9964, df = 728.15, p-value = 0.04627\nalternative hypothesis: true difference in means between group el_nino and group la_nina is not equal to 0\n95 percent confidence interval:\n -10.99248229  -0.09197763\nsample estimates:\nmean in group el_nino mean in group la_nina \n             281.9139              287.4562 \n\n\nWe reject the null hypothesis at the alpha = 0.05 significance level.\n\n\n\nDiscussion\nIt is interesting to see that with a significance value of alpha = 0.05, the difference in wave height was not statistically significant (but still close). However, the mean period was significantly smaller for the La Niña period than the El Niño, AND the mean swell direction was significantly steeper (more northerly, and less favorable for surfing in Santa Barbara) during the La Niña event. This lends support to the observation that El Niño years have bigger waves, longer periods, and a better swell direction (closer to 270 degrees) than La Niña years.\nHowever, this is still relatively inconclusive, because no two ENSO cycles are exactly alike and we only have one of each here to compare. It is likely that different ENSO events have slightly different qualities and it would be irresponsible to generalize that just from comparing two of them we have uncovered some absolute truths. Nonetheless, comparing these two events did line up with what I outlined in the “motivation” section above. Due to the altering of the pacific storm track during an El Niño event, we see more waves coming straight into the SB channel (~270 degrees, straight West) without having to wrap around Point Conception to the North.\nIn further research, I would look into data from the West Santa Barbara buoy. This location is far more protected from north and south swells, which could be useful for more specific stats questions pertaining to just west swells. However, the refraction of swell around the islands and Point Conception sucks a lot of energy out of the swell which could decrease the variability in our data, therefore making statistical inferences more subtle.\nIn conclusion, that results I got here were probably known by many experts and most surfers via their own anecdotal evidence. Either way, it was still really cool to find evidence through my favorite buoy that supported my own real-life observations.\n\n\nReferences\nArroyo, M., Levine, A., & Espejel, I. (2019). A transdisciplinary framework proposal for surf break conservation and management: Bahía de Todos Santos World Surfing Reserve. Ocean & Coastal Management, 168, 197–211. https://doi.org/10.1016/j.ocecoaman.2018.10.022\nBuckley, R. (2017). Perceived Resource Quality as a Framework to Analyze Impacts of Climate Change on Adventure Tourism: Snow, Surf, Wind, and Whitewater. Tourism Review International, 21(3), 241–254. https://doi.org/10.3727/154427217X15022104437729\nEl Nino, La Nina, ENSO and What They Mean to Your Surf. (2020, April 17). Surfline. https://www.surfline.com/surf-news/el-nino-la-nina-enso-what-they-mean-to-your-surf/82991\nGoldman, S. (2017, March 1). Storms, Powerful Waves Have Eaten Away Santa Barbara County Coastlines to Historic Levels. https://www.noozhawk.com/article/storms_waves_coastal_erosion_santa_barbara_county_historic_levels\nHow will climate change change El Niño and La Niña? - Welcome to NOAA Research. (2020, November 9). How Will Climate Change Change El Niño and La Niña? https://research.noaa.gov/article/ArtMID/587/ArticleID/2685/New-research-volume-explores-future-of-ENSO-under-influence-of-climate-change\nJohnson, N. (2022, November 22). Another winter in La Niña’s grip? – November update to NOAA’s 2022-23 Winter Outlook | NOAA Climate.gov. http://www.climate.gov/news-features/blogs/another-winter-la-ni%C3%B1a%E2%80%99s-grip-%E2%80%93-november-update-noaas-2022-23-winter-outlook\nNull, J. (2022, October 1). El Niño and La Niña Years and Intensities. https://ggweather.com/enso/oni.htm"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html",
    "href": "Posts/marine_aquaculture_geospatial/index.html",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "",
    "text": "source: https://fieldnotesjournal.org/new-blog/apearlintheroughoysteraquacultureandhowitworks"
  },
  {
    "objectID": "Posts/marine_aquaculture_geospatial/index.html#overview",
    "href": "Posts/marine_aquaculture_geospatial/index.html#overview",
    "title": "Study on Marine Aquaculture with Geospatial Ocean Data",
    "section": "Overview",
    "text": "Overview\nMarine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al.\nFor this small project, we will determine which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nsea surface temperature: 11-30 deg C\ndepth: 0-70 meters below sea level\n\n\nUtilized skills:\n\ncombining vector raster data\nresampling raster data\nmasking raster data\nmap algebra\n\n\n\nData\n\nSea Surface Temperature\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).2\n\n\nExclusive Economic Zones\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\n\n\nPrepare data\nTo start, we need to load all necessary data and make sure it has the coordinate reference system.\nRead in the shapefile for the West Coast EEZ (wc_regions_clean.shp)\n\n\nShow Code\nwc_regions <- st_read(here(file_path, \"data/wc_regions_clean.shp\"))\n\n\nReading layer `wc_regions_clean' from data source \n  `/Users/jaredpetry/Documents/MEDS/quarto_website/jaredbpetry.github.io/Posts/marine_aquaculture_geospatial/data/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\nRead in sea surface temperature rasters and combine them into a raster stack\n\n\nShow Code\n# use list.files() to read in our data \n#--- you start with a bunch of tif files that you want to stack\n#--- I created a list of just the ones starting with the letter \"a\" to get the sst rasters I wanted\nfile_list <- list.files(path = \"data/\", pattern = \"^[a]\", full.names = TRUE)\n#--- now read them in using rast... this will create a spatraster with multiple layers\n#--- (this isn't an actual raster stack so let's see if this works.. terra calls them the same thing)\nsst_spatrast <- rast(file_list)\n#--- at this point you get a crs that says epsg4326 AND epsg9122 so we'll have to change that\n\n\nRead in bathymetry raster (depth.tif)\n\n\n\nShow Code\ndepth <- rast(here(file_path, \"data/depth.tif\"))\n\n\nCheck that data are in the same coordinate reference system and reproject any data not in the same projection\n\n\nShow Code\n#--- depth raster is in lon/lat wgs84 EPSG:4326\n#--- wc_regions polygons in wgs84\n#--- sst_spatraster says lon/lat wgs84\n#depth <- st_transform(depth, crs = crs(sst_spatrast))\n\nset.crs(sst_spatrast, \"EPSG:4326\")\nset.crs(depth, \"EPSG:4326\")\nst_transform(wc_regions, crs = crs(depth)) #--- great now they are same crs\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key      area_m2 rgn_id  area_km2\n1              Oregon      OR 179994061293      1 179994.06\n2 Northern California    CA-N 164378809215      2 164378.81\n3  Central California    CA-C 202738329147      3 202738.33\n4 Southern California    CA-S 206860777840      4 206860.78\n5          Washington      WA  66898309678      5  66898.31\n                        geometry\n1 MULTIPOLYGON (((-123.4318 4...\n2 MULTIPOLYGON (((-124.2102 4...\n3 MULTIPOLYGON (((-122.9928 3...\n4 MULTIPOLYGON (((-120.6505 3...\n5 MULTIPOLYGON (((-122.7675 4...\n\n\n\n\nProcess data\nNext, we need process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\nFind the mean SST from 2008-2012\n\n\nShow Code\nmean_sst_spatrast <- terra::app(sst_spatrast, mean) \n\n\nConvert SST data from Kelvin to Celsius\n\n\nShow Code\nmean_sst_C <- mean_sst_spatrast - 273.15\n\n\nCrop depth raster to match the extent of the SST raster\n\n\nShow Code\ndepth_cropped <- extend(depth, mean_sst_C)\n\n\nResample the NPP data to match the resolution of the SST data using the nearest neighbor approach\n\n\nShow Code\ndepth_cropped_resampled <- resample(depth_cropped, mean_sst_C, \n                                    method = \"near\")\n\n\nCheck that the depth and SST match in resolution, extent, and coordinate reference system… can the rasters be stacked?\n\n\nShow Code\n#--- try to stack the depth raster and the SST raster:\ndepth_and_sst <- raster::stack(c(depth_cropped_resampled, mean_sst_C))\n#--- yes! they can be stacked\n\n\n\n\nFind suitable locations\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth.\nReclassify SST and depth data into locations that are suitable for oysters. We will do this by setting suitable values to 1 and unsuitable values to NA\n\n\nShow Code\n#--- for oysters, we need 11 <= sst(C) <= 30\n#------ and 0 <= depth <= 70\n\n#--- set up reclassification matrices \n\ndepth_rcl <- matrix(c(-8000, -70, NA,\n                    -70, 0, 1,\n                    0, 5000, NA), \n                 ncol = 3, byrow = TRUE)\n\nsst_rcl <- matrix(c(-Inf, 11, NA, \n                    11, 30, 1, \n                    30, Inf, NA), \n                 ncol = 3, byrow = TRUE)\n\n#--- use these to classify your rasters\n\nsuitable_sst <- classify(mean_sst_C, rcl = sst_rcl)\n\nsuitable_depth <- classify(depth_cropped_resampled, rcl = depth_rcl)\n\n#--- Plot what you just made for each \n\nraster::plot(suitable_depth, col = \"blue\") \n\n\n\n\n\nShow Code\nraster::plot(suitable_sst, col = \"blue\")\n\n\n\n\n\nFind locations that satisfy both SST and depth conditions\n- create an overlay using the lapp() function multiplying cell values - we will use terra::lapp()\n\n\nShow Code\n#--- we want to multiply cell values so that with any NA in either layer, result will also be NA, with two 1s, the result will also be a 1\n\nfun = function(x,y) {\n  return(x*y)\n}\n\noyster_habitat <- lapp(c(suitable_depth, suitable_sst), fun)\n#--- plot(oyster_habitat)\n\n\n\n\nDetermine the most suitable EEZ\nWe want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nselect suitable cells within West Coast EEZs\nfind area of grid cells\nfind the total suitable area within each EEZ\n\nhint: it might be helpful to rasterize the EEZ data\n\nfind the percentage of each zone that is suitable\n\nhint it might be helpful to join the suitable area by region onto the EEZ vector data\n\n\n\n\nShow Code\n#--- remember, our eez regions are the wc_regions we reprojected earlier\n\n# select suitable cells within West Coast EEZs\n#----first rasterize the wc_regions data to create rast_eez\nrast_regions <- terra::rasterize(wc_regions, oyster_habitat, field = \"rgn\")\n\n# compute the area covered by the raster cells of suitable habitat\n#--- terra::cellSize() will compute the area covered by each individual cell\narea_habitat <- cellSize(oyster_habitat, unit = \"km\", transform = TRUE)\n\n#--- create mask that will display the suitable habitat separated into the different eez regions\nmask <- mask(rast_regions, oyster_habitat)\n\n#--- terra::zonal() will compute summaries of values of a spatraster defined by the \"zones\" of a different spatraster.  We will use this to compute the suitable area \nhabitat_area <- terra::zonal(area_habitat, mask, sum)\n\n#--- use left_join() to create a dataframe that contains both the suitable area by region and the percentage of habitat area out of the total area for that region\nhabitat_by_region_df <- left_join(wc_regions, habitat_area, by = \"rgn\")  \nhabitat_by_region_df$area_percent = (habitat_by_region_df$area / habitat_by_region_df$area_km2)*100\n\n#--- show results in a table\nprint_df <- habitat_by_region_df |> \n  terra::as.data.frame() |> \n  dplyr::select(rgn, area, area_percent) |> \n  dplyr::rename(\"Oyster Habitat Area\" = area, \n                \"EEZ region\" = rgn, \n                \"Habitat Percent of Total Region Area\" = area_percent) \nprint_df\n\n\n           EEZ region Oyster Habitat Area Habitat Percent of Total Region Area\n1              Oregon           1074.2720                            0.5968374\n2 Northern California            178.0268                            0.1083028\n3  Central California           4069.8766                            2.0074530\n4 Southern California           3757.2849                            1.8163351\n5          Washington           2378.3137                            3.5551178\n\n\n\n\nVisualize results\nNow that we have results, we need to present them!\nWe will create the following maps:\n\ntotal suitable area by region\npercent suitable area by region\n\n\n\nShow Code\n#--- map of total area \narea_map <- tm_shape(habitat_by_region_df) + \n  tm_polygons(col = \"area\",\n              palette = rev(hcl.colors(3, \"BluGrn\")),\n              title = \"Habitat Area (square km)\",\n              legend.reverse = TRUE) +\ntm_shape(oyster_habitat) +\n  tm_raster(title = \"Habitat\") +\ntm_layout(legend.outside = TRUE,\n          main.title.size = 1,\n          main.title = \"Suitable habitat area for oysters by EEZ region\")\n\narea_map\n\n\n\n\n\nShow Code\n#--- map of percentage\npercent_map <- tm_shape(habitat_by_region_df) +\n  tm_polygons(col = \"area_percent\",\n              palette = rev(hcl.colors(3, \"BluGrn\")),\n              title = \"Percentage\") +\ntm_shape(oyster_habitat) +\n  tm_raster(title = \"Habitat\") +\ntm_layout(legend.outside = TRUE,\n          main.title.size = 1,\n          main.title = \"Suitable habitat for oysters: percentage of total EEZ region\",\n          frame = T)\npercent_map\n\n\n\n\n\n\n\nBroaden the workflow!\nNow that we’ve worked through the suitable habitat areas for one group of species, let’s update our workflow to work for other species. We will do this by creating a function that would allow you to reproduce your results for other species. It will be able to do the following:\n\naccept temperature and depth ranges and species name as inputs\ncreate maps of total suitable area and percent suitable area per EEZ with the species name in the title\n\n\n\nShow Code\nfind_suitable_habitat <- function(spp_name, temp_min, temp_max, depth_min, depth_max) {\n  depth_rcl <- matrix(c(-Inf, depth_min, NA, \n                               depth_min, depth_max, 1, \n                               depth_max, Inf, NA), \n                              ncol = 3, byrow = TRUE)\n  sst_rcl <- matrix(c(-Inf, temp_min, NA, \n                             temp_min, temp_max, 1, \n                             temp_max, Inf, NA), \n                           ncol = 3, byrow = TRUE)\n  suitable_sst <- classify(mean_sst_C, rcl = sst_rcl)\n  suitable_depth <- classify(depth_cropped_resampled, rcl = depth_rcl)\n  oyster_habitat <- lapp(c(suitable_depth, suitable_sst), fun)\n  rast_regions <- terra::rasterize(wc_regions, oyster_habitat, field = \"rgn\")\n  area_suitable <- cellSize(oyster_habitat, unit = \"km\", transform = TRUE)\n  mask <- mask(rast_regions, oyster_habitat)\n  area_per_zone <- terra::zonal(area_suitable, mask, sum)\n  df_habitat_rgn <- left_join(wc_regions, area_per_zone, by = \"rgn\") \n  df_habitat_rgn$area_percent <- (df_habitat_rgn$area / df_habitat_rgn$area_km2)*100\n  #--- map of total area \n  area_and_percent_map <- tmap_arrange(tm_shape(df_habitat_rgn) + \n    tm_polygons(col = \"area\",\n                palette = rev(hcl.colors(3, \"BluGrn\")),\n                title = \"Area(km sq.)\",\n                legend.reverse = TRUE) +\n    tm_shape(oyster_habitat) +\n      tm_raster(title = \"Habitat\") +\n    tm_layout(legend.outside = TRUE,\n              legend.title.size = 0.8,\n              legend.text.size = 0.5,\n              main.title.size = 0.8,\n              main.title = paste0(spp_name, \" habitat by EEZ region\")),\n    percent_map <- tm_shape(df_habitat_rgn) +\n      tm_polygons(col = \"area_percent\",\n                  palette = rev(hcl.colors(3, \"BluGrn\")),\n                  title = \"Percentage\") +\n    tm_shape(oyster_habitat) +\n      tm_raster(title = \"Habitat\") +\n    tm_layout(legend.outside = TRUE,\n              legend.title.size = 0.8,\n              legend.text.size = 0.5,\n              main.title.size = 0.8,\n              main.title = paste0(spp_name, \" habitat percentage of EEZ region\"),\n              frame = T))\n  area_and_percent_map\n}\n#--- test the function (arbitrary values) \nfind_suitable_habitat(spp_name = \"spp of interest\", 5, 15, -100, 0)\n\n\n\n\n\nAnyone who wants to reproduce this workflow can do so easily! Run your function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase. Remember, we are thinking about the potential for marine aquaculture, so these species should have some reasonable potential for commercial consumption.\nI chose the American Lobster because it is a highly sought after food that could return lots of profit if grown in the right place. I used to work at a fish market and it was extremely hard to get lobster because it is so hard to get nowadays! The temperature range is 11-19 degC and the depth range is sea level to 480 meters deep.\n\n\nShow Code\nfind_suitable_habitat(spp_name = \"American lobster\", 11, 19, -480, 0)\n\n\n\n\n\n\n\nSB depth data… just for fun\nCan I do a SB analysis with the depth data? I was going to try this to see bathymetry data for surf spor intel… however the data wasn’t high enough resolution. Oh well it still looks cool at least\n\n\nShow Code\nrast_to_crop <- depth\nlat_up <- 34.522\nlat_down <- 34.085\nlon_up <- -120.223\nlon_down <- -119.171\n#--- formatting for ext(): -180, 180, -90, 90 (xmin, xmax, ymin, ymax)\nextent_of_crop <- terra::ext(lon_up, lon_down, lat_down, lat_up)\nsb_depth <- crop(rast_to_crop, extent_of_crop)\n\n#--- also crop the basemap to the same thing\n#sb_basemap <- extend(wc_regions, sb_depth)\n\ntm_shape(sb_depth) + tm_raster()"
  }
]