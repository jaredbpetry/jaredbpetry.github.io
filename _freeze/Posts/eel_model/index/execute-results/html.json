{
  "hash": "5967a7dd511a1497200d6243ca0a3bbd",
  "result": {
    "markdown": "---\ntitle: \"Tuning an XGBoost Machine Learning Model to Predict Eel Presence\"\ndescription: |\n  \"Using a dataset with a variety of physical and atmospheric habitat variables train a boosted tree classification model the predict the presence or absence of the short finned eel\"\nauthor:\n  - name: Jared Petry \n    url: https://jaredbpetry.github.io\n    affiliation: Master of Environmental Data Science Program at UCSB\ndate: 2023-02-01\nbibliography: references.bib\ncatagories: [MEDS, R, geospatial, machine learning, modeling, boosted regression trees]\n#citation: \n  #url:\n  #photo:\nimage: eel.png\ndraft: false\nformat: \n  html: \n    code-fold: false\n    code-summary: \"Show Code\"\n    toc: true\n    toc-depth: 6 \n    toc-title: Contents\ncode-overflow: wrap\ncode-block-bg: true\ncode-block-border-left: \"#6B5A75\"\neditor: visual\n---\n\n\n## Case Study: Eel Species Distribution Modeling\n\nThis blog post loosely follows the boosted regression tree exercise outlined in the academic paper: \"A working guide to boosted regression trees\" by Edith et al. We will use the same dataset that they did on the distribution of the short finned eel (Anguilla australis). We will be using the xgboost library, tidymodels, caret, parsnip, vip, and more.\n\nCitation:\n\nElith, J., Leathwick, J. R., & Hastie, T. (2008). A working guide to boosted regression trees. Journal of Animal Ecology, 77(4), 802–813. https://doi.org/10.1111/j.1365-2656.2008.01390.x\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.0     ✔ rsample      1.1.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.3     ✔ workflows    1.0.0\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.1     ✔ yardstick    1.1.0\n✔ recipes      1.0.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n\n```{.r .cell-code}\nlibrary(gbm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded gbm 2.1.8.1\n```\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggpubr)\nlibrary(tictoc)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n:::\n:::\n\n\nRead in the data\n\n::: {.cell}\n\n```{.r .cell-code}\neel_dat <- read_csv(\"eel.model.data.csv\") |> \n  mutate(Angaus = as.factor(Angaus))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Method\ndbl (13): Site, Angaus, SegSumT, SegTSeas, SegLowFlow, DSDist, DSMaxSlope, U...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\n\n### Split and Resample\n\nSplit the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create initial split\neel_split <- initial_split(data = eel_dat, \n                           prop = 0.7, \n                           strata = Angaus)\n\n# create training and testing data from the split \neel_train <- training(eel_split) \neel_test <- testing(eel_split)\n\n# resample the training set \neel_folds <- vfold_cv(data = eel_train, \n                      v = 10,\n                      strata = Angaus)\n```\n:::\n\n\n\n### Preprocess\n\nCreate a recipe to prepare the data for the XGBoost model.  We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neel_recipe <- recipe(Angaus ~ ., data = eel_train) |> \n  step_integer(all_predictors(), zero_based = TRUE) \n```\n:::\n\n\n\n## Tuning XGBoost\n\nWe are going to tune 3 different times to get the ideal hyperparamters in our model.\nWe first tune the learning rate and get the estimation of the best learning rate. \nThen we take that learning rate and set it as fixed in the next tuning.\nNext, we tune the three tree paramters: tree_depth, loss_reduction, and min_n.\nLastly, we will set those as fixed for the stochastic tuning (m_try).\n\n### Tune Learning Rate\n\n1.  Create a model specification using {xgboost} for the estimation\n-   Only specify learning rate parameter to tune()\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create model specification: (tune learning rate first)\neel_boost_model <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = NULL,\n  loss_reduction = NULL, \n  learn_rate = tune(),\n  min_n = NULL,\n  mtry = NULL\n  )\n```\n:::\n\n\n\n2.  Set up a grid to tune the model by using a range of learning rate parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearn_rate_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n```\n:::\n\n\nComputational efficiency becomes a factor as models get more complex and data gets larger. Becuase of this, we will be recording the time it takes to run with `Sys.time()`.\n\nTune the learning rate parameter:\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune the learning rate:\n\n# define a workflow for tuning the learning rate: \neel_learn_wf <- workflow() |> \n  add_model(eel_boost_model) |> \n  add_recipe(eel_recipe)\n\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_learn <- eel_learn_wf |> \n  tune_grid(\n    eel_folds, \n    grid = learn_rate_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"time elapsed: 5.74493848482768\"\n```\n:::\n:::\n\n\n\n3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(tune_learn, metric = \"roc_auc\")  # using metric roc_auc \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  learn_rate .metric .estimator  mean     n std_err .config              \n       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1      0.3   roc_auc binary     0.827    10  0.0204 Preprocessor1_Model30\n2      0.155 roc_auc binary     0.824    10  0.0178 Preprocessor1_Model16\n3      0.104 roc_auc binary     0.823    10  0.0157 Preprocessor1_Model11\n4      0.207 roc_auc binary     0.823    10  0.0177 Preprocessor1_Model21\n5      0.279 roc_auc binary     0.822    10  0.0171 Preprocessor1_Model28\n```\n:::\n\n```{.r .cell-code}\n# --- learn rate = 0.04146552... use this because the paper did\nshow_best(tune_learn, metric = \"accuracy\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  learn_rate .metric  .estimator  mean     n std_err .config              \n       <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1      0.3   accuracy binary     0.824    10  0.0107 Preprocessor1_Model30\n2      0.290 accuracy binary     0.822    10  0.0144 Preprocessor1_Model29\n3      0.124 accuracy binary     0.822    10  0.0139 Preprocessor1_Model13\n4      0.259 accuracy binary     0.822    10  0.0147 Preprocessor1_Model26\n5      0.186 accuracy binary     0.821    10  0.0132 Preprocessor1_Model19\n```\n:::\n:::\n\n\n\n### Tune Tree Parameters\n\n1.  Create a new specification where you set the learning rate (which we already optimized) and tune the tree parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create model specification for tuning tree depth with new (optimized) learning rate\neel_boost_model2 <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = tune(),\n  loss_reduction = tune(), \n  learn_rate = 0.04146552,\n  min_n = tune(),\n  mtry = NULL\n  )\n\n# define workflow for tuning tree parameters: \ntree_tune_wf <- workflow() |> \n  add_model(eel_boost_model2) |> \n  add_recipe(eel_recipe)\n```\n:::\n\n\n\n2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we are now tuning the tree parameters: tree_depth(), min_n(), and loss_reduction()\n\n# use tidymodels dials package to specify which paramters we are trying to tune... \n# --- grid_max_entropy() needs an object like this to work properly\n\ntree_depth_param <- dials::parameters(\n  tree_depth(), \n  min_n(), \n  loss_reduction()\n)\ntree_tune_grid <- grid_max_entropy(tree_depth_param, size = 60)\n```\n:::\n\n\n\n3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_tree <- tree_tune_wf |> \n  tune_grid(\n    eel_folds, \n    grid = tree_tune_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"time elapsed: 9.92073663473129\"\n```\n:::\n\n```{.r .cell-code}\nshow_best(tune_tree, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  min_n tree_depth loss_reduction .metric .estimator  mean     n std_err .config\n  <int>      <int>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1     6          2          5.13  roc_auc binary     0.859    10  0.0188 Prepro…\n2    12         15          6.03  roc_auc binary     0.855    10  0.0198 Prepro…\n3     3          8          4.04  roc_auc binary     0.844    10  0.0189 Prepro…\n4    13         13          0.964 roc_auc binary     0.843    10  0.0191 Prepro…\n5    21         15          3.74  roc_auc binary     0.841    10  0.0196 Prepro…\n```\n:::\n\n```{.r .cell-code}\n# best value for min_n: 17\n# best value for tree_depth: 11\n# best value for loss_reduction: 0.367\n```\n:::\n\n\n\n### Tune Stochastic Parameters\n\n1.  We will create a new specification where we set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters (m_try).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model specification with fixed learning rate and tree parameters to tune stochastic params\neel_boost_model3 <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = 11,\n  loss_reduction = 0.367, \n  learn_rate = 0.04146552,\n  min_n = 17,\n  mtry = tune(), \n  sample_size = tune()\n  )\n\n# define workflow for tuning stochastic parameters: \nstoch_tune_wf <- workflow() |> \n  add_model(eel_boost_model3) |> \n  add_recipe(eel_recipe)\n```\n:::\n\n\n\n2.  Set up a tuning grid using grid_max_entropy() again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstoch_param <- dials::parameters(\n  finalize(mtry(), select(eel_train, -Angaus)), \n  sample_size = sample_prop(c(0.4, 0.9))\n)\nstoch_grid <- grid_max_entropy(stoch_param, size = 60)\n```\n:::\n\n\n\n3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_stoch <- stoch_tune_wf |> \n  tune_grid(\n    eel_folds, \n    grid = stoch_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"time elapsed: 4.0953741541836\"\n```\n:::\n\n```{.r .cell-code}\nshow_best(tune_stoch, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   mtry sample_size .metric .estimator  mean     n std_err .config              \n  <int>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    10       0.884 roc_auc binary     0.835    10  0.0204 Preprocessor1_Model60\n2     5       0.839 roc_auc binary     0.834    10  0.0209 Preprocessor1_Model07\n3     2       0.865 roc_auc binary     0.833    10  0.0212 Preprocessor1_Model14\n4     8       0.851 roc_auc binary     0.833    10  0.0201 Preprocessor1_Model01\n5     4       0.870 roc_auc binary     0.832    10  0.0219 Preprocessor1_Model51\n```\n:::\n\n```{.r .cell-code}\n# best value for mtry: 1\n# best value for sample size: 0.8932816\n```\n:::\n\n\n\n## Finalize workflow and make final prediction\n\n1.  Assemble your final workflow with all of your optimized parameters and do a final fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model specification with optimal parameters\neel_boost_final <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = 11,\n  loss_reduction = 0.367, \n  learn_rate = 0.04146552,\n  min_n = 17,\n  mtry = 1, \n  sample_size = 0.8932816\n  )\n\n# define workflow for final fit\nstoch_tune_wf <- workflow() |> \n  add_model(eel_boost_final) |> \n  add_recipe(eel_recipe)\n\nfinal_eel_fit <- last_fit(eel_boost_final, \n                          Angaus ~ ., \n                          eel_split)\n\nfinal_eel_fit$.predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 301 × 6\n   .pred_0 .pred_1  .row .pred_class Angaus .config             \n     <dbl>   <dbl> <int> <fct>       <fct>  <chr>               \n 1  0.123  0.877       2 1           1      Preprocessor1_Model1\n 2  0.940  0.0598      5 0           1      Preprocessor1_Model1\n 3  0.986  0.0144      6 0           0      Preprocessor1_Model1\n 4  0.993  0.00708    14 0           0      Preprocessor1_Model1\n 5  0.959  0.0408     21 0           0      Preprocessor1_Model1\n 6  0.0648 0.935      23 1           1      Preprocessor1_Model1\n 7  0.711  0.289      24 0           0      Preprocessor1_Model1\n 8  0.985  0.0155     27 0           0      Preprocessor1_Model1\n 9  0.873  0.127      39 0           0      Preprocessor1_Model1\n10  0.970  0.0300     41 0           0      Preprocessor1_Model1\n# … with 291 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n```{.r .cell-code}\nboost_tree_metrics <- final_eel_fit |> collect_metrics()\nboost_tree_accuracy <- boost_tree_metrics$.estimate[1]\nprint(paste0(\"the decision tree model accuracy came out to: \", boost_tree_accuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"the decision tree model accuracy came out to: 0.827242524916944\"\n```\n:::\n:::\n\n\n2. How well did your model perform? What types of errors did it make?\n\nmake a confusion matrix\n\n::: {.cell}\n\n```{.r .cell-code}\n# to make a confusion matrix we need a table of the predictions vs the true values\nboost_predictions <- final_eel_fit$.predictions[[1]]\n\n# make a simple table of just the predictions and actual values\nconfusion_table <- boost_predictions |> \n  select(c(.pred_class, Angaus))\n\n# create a confusion matrix comparing the predictions with actual observations\nconfusionMatrix(data = confusion_table$.pred_class, \n                reference = confusion_table$Angaus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 221  33\n         1  19  28\n                                          \n               Accuracy : 0.8272          \n                 95% CI : (0.7797, 0.8682)\n    No Information Rate : 0.7973          \n    P-Value [Acc > NIR] : 0.10999         \n                                          \n                  Kappa : 0.4154          \n                                          \n Mcnemar's Test P-Value : 0.07142         \n                                          \n            Sensitivity : 0.9208          \n            Specificity : 0.4590          \n         Pos Pred Value : 0.8701          \n         Neg Pred Value : 0.5957          \n             Prevalence : 0.7973          \n         Detection Rate : 0.7342          \n   Detection Prevalence : 0.8439          \n      Balanced Accuracy : 0.6899          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n**Well, looks like my model did OK.  It got an accuracy of 0.8339 which is not too bad... from looking at the confusion matrix, most of the errors that were made were false negative predictions, which were over twice the number of false positive predictions.**\n\n## Fit your model the evaluation data and compare performance\n\n1.  Now we will fit the final model to the big dataset used in the paper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the eval data \neval_data <- read_csv(\"eel.eval.data.csv\") |> \n  mutate(Angaus_obs = as.factor(Angaus_obs))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 500 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Method\ndbl (12): Angaus_obs, SegSumT, SegTSeas, SegLowFlow, DSDist, DSMaxSlope, USA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\n# finalize our model: \nbest_params <- select_best(tune_stoch)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n```{.r .cell-code}\nfinal_model <- finalize_model(eel_boost_final, parameters = best_params)\n\n# make predictions with our model\neval_fit <- final_model |> fit(Angaus_obs ~ ., data = eval_data)\n```\n:::\n\n\n2.  How does the model perform on this data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate the predicted outcomes \neval_preds <- eval_fit |> predict(new_data = eval_data)\neval_pred_probs <- eval_fit |> predict_classprob.model_fit(new_data = eval_data)\njoined_predictions <- bind_cols(eval_data, eval_preds, eval_pred_probs)\n\n# assess model performance with a confusion matrix \nconfusionMatrix(data = joined_predictions$.pred_class, \n                reference = joined_predictions$Angaus_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 369  54\n         1  24  53\n                                          \n               Accuracy : 0.844           \n                 95% CI : (0.8092, 0.8747)\n    No Information Rate : 0.786           \n    P-Value [Acc > NIR] : 0.0006588       \n                                          \n                  Kappa : 0.4836          \n                                          \n Mcnemar's Test P-Value : 0.0010249       \n                                          \n            Sensitivity : 0.9389          \n            Specificity : 0.4953          \n         Pos Pred Value : 0.8723          \n         Neg Pred Value : 0.6883          \n             Prevalence : 0.7860          \n         Detection Rate : 0.7380          \n   Detection Prevalence : 0.8460          \n      Balanced Accuracy : 0.7171          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n**Woohoo! we got an even better accuracy than with the testing data that we fit the model to!**\n**Accuracy was 84.4% Still over twice as many false negative errors than false positive ones**\n\n3.  How do our results compare to those of Elith et al.?\n\n> As for variable importance, I got just about the same results for the most important predictors being: summer air temp, distance to coast, native vegetation, downstream max slope... execpt, in the paper one of the most important was the fishing method... which didn't pop up for me which I thought was interesting. The roc area under the curve that the scientists from the paper acheived was 0.858, which was slightly higher than mine... dang! But at least it was really close.\n\n## Variable Importance\n\nUsing the package {vip} to compare variable importance. This is really cool because we can see which variables influenced the model the most once it is finalized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a plot of variable importance\nvip(eval_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n## Discussion\n\nWhat do our variable importance results tell us about the distribution of this eel species?\n\n> Summer air temperature is very important to this species because it was the most influential variable in the model. Maybe some part of their breeding or other important stage of their life cycle occurs in summer. From the code I wrote below, it seems like they like warmer temperatures on average in the summer. \n\n> Their distance to the coast is also very important in determining the prescence of this eel species. The average distance for the dataset while the eel is present is HALF of that with the eel absent, so I'm thinking that the eel likes to be closer to the coast rather than farther. Maybe this is because they are anadromous in some way or need brackish water for part of their life cycle.\n\n> The species also seems to be heavily influence in the amount of native forest that the particular habitat contains... no surprise there! \n\n> The species also seems to like areas with more gently sloped downstream areas, which to me suggests that they somewhat rely on being able to travel up and downstream... \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a little subset dataframe for just the places that the eel was present\neel_pres <- eel_dat |> \n  filter(Angaus == 1)\neel_abs <- eel_dat |> \n  filter(Angaus == 0)\n\n# find out what summer temperature they like \nmean(eel_pres$SegSumT) # = 17.8005\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17.8005\n```\n:::\n\n```{.r .cell-code}\nmean(eel_abs$SegSumT) # = 16.8005\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16.07343\n```\n:::\n\n```{.r .cell-code}\n# find out if they like to be closer or farther from the coast\nmean(eel_pres$DSDist) # = 42.47604\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 42.47604\n```\n:::\n\n```{.r .cell-code}\nmean(eel_abs$DSDist) # = 82.60588\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 82.60588\n```\n:::\n\n```{.r .cell-code}\n# find out whether they like steeper slope or shallower slope \nmean(eel_pres$DSMaxSlope) # = 1.544\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.544554\n```\n:::\n\n```{.r .cell-code}\nmean(eel_abs$DSMaxSlope) # = 3.395\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.395376\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}