{
  "hash": "e7376ec41505419b01e89b0cbaa08612",
  "result": {
    "markdown": "---\ntitle: \"Tuning an XGBoost Machine Learning Model to Predict Eel Presence\"\ndescription: |\n  \"Using a dataset with a variety of physical and atmospheric habitat variables train a boosted tree classification model the predict the presence or absence of the short finned eel\"\nauthor:\n  - name: Jared Petry \n    url: https://jaredbpetry.github.io\n    affiliation: Master of Environmental Data Science Program at UCSB\ndate: 2023-02-01\n#bibliography: references.bib\ncatagories: [MEDS, R, machine learning, modeling, boosted regression trees]\n#citation: \n  #url:\n  #photo:\nimage: eel.png\ndraft: false\nformat: \n  html: \n    code-fold: false\n    code-summary: \"Show Code\"\n    toc: true\n    toc-depth: 6 \n    toc-title: Contents\ncode-overflow: wrap\ncode-block-bg: true\ncode-block-border-left: \"#6B5A75\"\neditor: visual\n---\n\n\n## Case Study: Eel Species Distribution Modeling\n\nThis blog post loosely follows the boosted regression tree exercise outlined in the academic paper: \"A working guide to boosted regression trees\" by Edith et al. We will use the same dataset that they did on the distribution of the short finned eel (Anguilla australis). We will be using the xgboost library, tidymodels, caret, parsnip, vip, and more.\n\nCitation:\n\nElith, J., Leathwick, J. R., & Hastie, T. (2008). A working guide to boosted regression trees. Journal of Animal Ecology, 77(4), 802–813. https://doi.org/10.1111/j.1365-2656.2008.01390.x\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.0     ✔ rsample      1.1.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.3     ✔ workflows    1.0.0\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.1     ✔ yardstick    1.1.0\n✔ recipes      1.0.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n\n```{.r .cell-code}\nlibrary(gbm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded gbm 2.1.8.1\n```\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggpubr)\nlibrary(tictoc)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n:::\n:::\n\n\nRead in the data\n\n::: {.cell}\n\n```{.r .cell-code}\neel_dat <- read_csv(\"eel.model.data.csv\") |> \n  mutate(Angaus = as.factor(Angaus))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Method\ndbl (13): Site, Angaus, SegSumT, SegTSeas, SegLowFlow, DSDist, DSMaxSlope, U...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\n\n### Split and Resample\n\nSplit the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create initial split\neel_split <- initial_split(data = eel_dat, \n                           prop = 0.7, \n                           strata = Angaus)\n\n# create training and testing data from the split \neel_train <- training(eel_split) \neel_test <- testing(eel_split)\n\n# resample the training set \neel_folds <- vfold_cv(data = eel_train, \n                      v = 10,\n                      strata = Angaus)\n```\n:::\n\n\n\n### Preprocess\n\nCreate a recipe to prepare the data for the XGBoost model.  We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neel_recipe <- recipe(Angaus ~ ., data = eel_train) |> \n  step_integer(all_predictors(), zero_based = TRUE) \n```\n:::\n\n\n\n## Tuning XGBoost\n\nWe are going to tune 3 different times to get the ideal hyperparamters in our model.\nWe first tune the learning rate and get the estimation of the best learning rate. \nThen we take that learning rate and set it as fixed in the next tuning.\nNext, we tune the three tree paramters: tree_depth, loss_reduction, and min_n.\nLastly, we will set those as fixed for the stochastic tuning (m_try).\n\n### Tune Learning Rate\n\n1.  Create a model specification using {xgboost} for the estimation\n-   Only specify learning rate parameter to tune()\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create model specification: (tune learning rate first)\neel_boost_model <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = NULL,\n  loss_reduction = NULL, \n  learn_rate = tune(),\n  min_n = NULL,\n  mtry = NULL\n  )\n```\n:::\n\n\n\n2.  Set up a grid to tune the model by using a range of learning rate parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearn_rate_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n```\n:::\n\n\nComputational efficiency becomes a factor as models get more complex and data gets larger. Becuase of this, we will be recording the time it takes to run with `Sys.time()`.\n\nTune the learning rate parameter:\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune the learning rate:\n\n# define a workflow for tuning the learning rate: \neel_learn_wf <- workflow() |> \n  add_model(eel_boost_model) |> \n  add_recipe(eel_recipe)\n\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_learn <- eel_learn_wf |> \n  tune_grid(\n    eel_folds, \n    grid = learn_rate_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"time elapsed: 5.83796453078588\"\n```\n:::\n:::\n\n\n\n3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(tune_learn, metric = \"roc_auc\")  # using metric roc_auc \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  learn_rate .metric .estimator  mean     n std_err .config              \n       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     0.0725 roc_auc binary     0.821    10  0.0214 Preprocessor1_Model08\n2     0.0518 roc_auc binary     0.820    10  0.0205 Preprocessor1_Model06\n3     0.104  roc_auc binary     0.820    10  0.0241 Preprocessor1_Model11\n4     0.269  roc_auc binary     0.819    10  0.0223 Preprocessor1_Model27\n5     0.207  roc_auc binary     0.819    10  0.0241 Preprocessor1_Model21\n```\n:::\n\n```{.r .cell-code}\n# --- learn rate = 0.04146552... use this because the paper did\nshow_best(tune_learn, metric = \"accuracy\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  learn_rate .metric  .estimator  mean     n std_err .config              \n       <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1     0.259  accuracy binary     0.823    10 0.00873 Preprocessor1_Model26\n2     0.124  accuracy binary     0.821    10 0.0112  Preprocessor1_Model13\n3     0.207  accuracy binary     0.821    10 0.0110  Preprocessor1_Model21\n4     0.176  accuracy binary     0.818    10 0.0118  Preprocessor1_Model18\n5     0.0104 accuracy binary     0.818    10 0.0123  Preprocessor1_Model02\n```\n:::\n:::\n\n\n\n### Tune Tree Parameters\n\n1.  Create a new specification where you set the learning rate (which we already optimized) and tune the tree parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create model specification for tuning tree depth with new (optimized) learning rate\neel_boost_model2 <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = tune(),\n  loss_reduction = tune(), \n  learn_rate = 0.04146552,\n  min_n = tune(),\n  mtry = NULL\n  )\n\n# define workflow for tuning tree parameters: \ntree_tune_wf <- workflow() |> \n  add_model(eel_boost_model2) |> \n  add_recipe(eel_recipe)\n```\n:::\n\n\n\n2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we are now tuning the tree parameters: tree_depth(), min_n(), and loss_reduction()\n\n# use tidymodels dials package to specify which paramters we are trying to tune... \n# --- grid_max_entropy() needs an object like this to work properly\n\ntree_depth_param <- dials::parameters(\n  tree_depth(), \n  min_n(), \n  loss_reduction()\n)\ntree_tune_grid <- grid_max_entropy(tree_depth_param, size = 60)\n```\n:::\n\n\n\n3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_tree <- tree_tune_wf |> \n  tune_grid(\n    eel_folds, \n    grid = tree_tune_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"time elapsed: 11.8651802341143\"\n```\n:::\n\n```{.r .cell-code}\nshow_best(tune_tree, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  min_n tree_depth loss_reduction .metric .estimator  mean     n std_err .config\n  <int>      <int>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1     3         14       4.98e- 9 roc_auc binary     0.831    10  0.0209 Prepro…\n2    15         11       2.75e+ 0 roc_auc binary     0.830    10  0.0187 Prepro…\n3     2         15       2.35e- 6 roc_auc binary     0.829    10  0.0202 Prepro…\n4     4          7       1.63e-10 roc_auc binary     0.828    10  0.0238 Prepro…\n5     3          9       1.48e- 5 roc_auc binary     0.827    10  0.0213 Prepro…\n```\n:::\n\n```{.r .cell-code}\n# best value for min_n: 17\n# best value for tree_depth: 11\n# best value for loss_reduction: 0.367\n```\n:::\n\n\n\n### Tune Stochastic Parameters\n\n1.  We will create a new specification where we set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters (m_try).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model specification with fixed learning rate and tree parameters to tune stochastic params\neel_boost_model3 <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = 11,\n  loss_reduction = 0.367, \n  learn_rate = 0.04146552,\n  min_n = 17,\n  mtry = tune(), \n  sample_size = tune()\n  )\n\n# define workflow for tuning stochastic parameters: \nstoch_tune_wf <- workflow() |> \n  add_model(eel_boost_model3) |> \n  add_recipe(eel_recipe)\n```\n:::\n\n\n\n2.  Set up a tuning grid using grid_max_entropy() again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstoch_param <- dials::parameters(\n  finalize(mtry(), dplyr::select(eel_train, -Angaus)), \n  sample_size = sample_prop(c(0.4, 0.9))\n)\nstoch_grid <- grid_max_entropy(stoch_param, size = 60)\n```\n:::\n\n\n\n3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit to the tuning grid \nstart_time1 <- Sys.time()\ntune_stoch <- stoch_tune_wf |> \n  tune_grid(\n    eel_folds, \n    grid = stoch_grid\n  )\nend_time1 <- Sys.time()\nprint(paste(\"time elapsed:\", (end_time1 - start_time1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"time elapsed: 20.4042701681455\"\n```\n:::\n\n```{.r .cell-code}\nshow_best(tune_stoch, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   mtry sample_size .metric .estimator  mean     n std_err .config              \n  <int>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     2       0.886 roc_auc binary     0.808    10  0.0214 Preprocessor1_Model07\n2     5       0.447 roc_auc binary     0.808    10  0.0257 Preprocessor1_Model32\n3     6       0.775 roc_auc binary     0.806    10  0.0242 Preprocessor1_Model40\n4     1       0.710 roc_auc binary     0.806    10  0.0241 Preprocessor1_Model60\n5     3       0.443 roc_auc binary     0.806    10  0.0249 Preprocessor1_Model10\n```\n:::\n\n```{.r .cell-code}\n# best value for mtry: 1\n# best value for sample size: 0.8932816\n```\n:::\n\n\n\n## Finalize workflow and make final prediction\n\n1.  Assemble your final workflow with all of your optimized parameters and do a final fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model specification with optimal parameters\neel_boost_final <- boost_tree(\n  mode = \"classification\", \n  trees = 3000, \n  engine = \"xgboost\", \n  tree_depth = 11,\n  loss_reduction = 0.367, \n  learn_rate = 0.04146552,\n  min_n = 17,\n  mtry = 1, \n  sample_size = 0.8932816\n  )\n\n# define workflow for final fit\nstoch_tune_wf <- workflow() |> \n  add_model(eel_boost_final) |> \n  add_recipe(eel_recipe)\n\nfinal_eel_fit <- last_fit(eel_boost_final, \n                          Angaus ~ ., \n                          eel_split)\n\nfinal_eel_fit$.predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 301 × 6\n   .pred_0 .pred_1  .row .pred_class Angaus .config             \n     <dbl>   <dbl> <int> <fct>       <fct>  <chr>               \n 1   0.194 0.806       2 1           1      Preprocessor1_Model1\n 2   0.812 0.188       4 0           0      Preprocessor1_Model1\n 3   0.986 0.0136      6 0           0      Preprocessor1_Model1\n 4   0.551 0.449      10 0           1      Preprocessor1_Model1\n 5   0.968 0.0321     11 0           0      Preprocessor1_Model1\n 6   0.996 0.00419    15 0           0      Preprocessor1_Model1\n 7   0.215 0.785      18 1           1      Preprocessor1_Model1\n 8   0.774 0.226      20 0           0      Preprocessor1_Model1\n 9   0.969 0.0314     22 0           0      Preprocessor1_Model1\n10   0.985 0.0155     28 0           0      Preprocessor1_Model1\n# … with 291 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n```{.r .cell-code}\nboost_tree_metrics <- final_eel_fit |> collect_metrics()\nboost_tree_accuracy <- boost_tree_metrics$.estimate[1]\nprint(paste0(\"the decision tree model accuracy came out to: \", boost_tree_accuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"the decision tree model accuracy came out to: 0.850498338870432\"\n```\n:::\n:::\n\n\n2. How well did your model perform? What types of errors did it make?\n\nmake a confusion matrix\n\n::: {.cell}\n\n```{.r .cell-code}\n# to make a confusion matrix we need a table of the predictions vs the true values\nboost_predictions <- final_eel_fit$.predictions[[1]]\n\n# make a simple table of just the predictions and actual values\nconfusion_table <- boost_predictions |> \n  dplyr::select(c(.pred_class, Angaus))\n\n# create a confusion matrix comparing the predictions with actual observations\nconfusionMatrix(data = confusion_table$.pred_class, \n                reference = confusion_table$Angaus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 226  31\n         1  14  30\n                                          \n               Accuracy : 0.8505          \n                 95% CI : (0.8051, 0.8888)\n    No Information Rate : 0.7973          \n    P-Value [Acc > NIR] : 0.01109         \n                                          \n                  Kappa : 0.4837          \n                                          \n Mcnemar's Test P-Value : 0.01707         \n                                          \n            Sensitivity : 0.9417          \n            Specificity : 0.4918          \n         Pos Pred Value : 0.8794          \n         Neg Pred Value : 0.6818          \n             Prevalence : 0.7973          \n         Detection Rate : 0.7508          \n   Detection Prevalence : 0.8538          \n      Balanced Accuracy : 0.7167          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n**Well, looks like my model did OK.  It got an accuracy of 0.8339 which is not too bad... from looking at the confusion matrix, most of the errors that were made were false negative predictions, which were over twice the number of false positive predictions.**\n\n## Fit the model to the evaluation data and compare performance\n\n1.  Now we will fit the final model to the big dataset used in the paper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the eval data \neval_data <- read_csv(\"eel.eval.data.csv\") |> \n  mutate(Angaus_obs = as.factor(Angaus_obs))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 500 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Method\ndbl (12): Angaus_obs, SegSumT, SegTSeas, SegLowFlow, DSDist, DSMaxSlope, USA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\n# finalize our model: \nbest_params <- select_best(tune_stoch)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n```{.r .cell-code}\nfinal_model <- finalize_model(eel_boost_final, parameters = best_params)\n\n# make predictions with our model\neval_fit <- final_model |> fit(Angaus_obs ~ ., data = eval_data)\n```\n:::\n\n\n2.  How does the model perform on this data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate the predicted outcomes \neval_preds <- eval_fit |> predict(new_data = eval_data)\neval_pred_probs <- eval_fit |> predict_classprob.model_fit(new_data = eval_data)\njoined_predictions <- bind_cols(eval_data, eval_preds, eval_pred_probs)\n\n# assess model performance with a confusion matrix \nconfusionMatrix(data = joined_predictions$.pred_class, \n                reference = joined_predictions$Angaus_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 374  57\n         1  19  50\n                                          \n               Accuracy : 0.848           \n                 95% CI : (0.8135, 0.8783)\n    No Information Rate : 0.786           \n    P-Value [Acc > NIR] : 0.0002819       \n                                          \n                  Kappa : 0.4811          \n                                          \n Mcnemar's Test P-Value : 2.194e-05       \n                                          \n            Sensitivity : 0.9517          \n            Specificity : 0.4673          \n         Pos Pred Value : 0.8677          \n         Neg Pred Value : 0.7246          \n             Prevalence : 0.7860          \n         Detection Rate : 0.7480          \n   Detection Prevalence : 0.8620          \n      Balanced Accuracy : 0.7095          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n**Woohoo! we got an even better accuracy than with the testing data that we fit the model to!**\n**Accuracy was 84.4% Still over twice as many false negative errors than false positive ones**\n\n3.  How do our results compare to those of Elith et al.?\n\n> As for variable importance, I got just about the same results for the most important predictors being: summer air temp, distance to coast, native vegetation, downstream max slope... execpt, in the paper one of the most important was the fishing method... which didn't pop up for me which I thought was interesting. The roc area under the curve that the scientists from the paper acheived was 0.858, which was slightly higher than mine... dang! But at least it was really close.\n\n## Variable Importance\n\nUsing the package {vip} to compare variable importance. This is really cool because we can see which variables influenced the model the most once it is finalized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a plot of variable importance\nvip(eval_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n## Discussion\n\nWhat do our variable importance results tell us about the distribution of this eel species?\n\n> Summer air temperature is very important to this species because it was the most influential variable in the model. Maybe some part of their breeding or other important stage of their life cycle occurs in summer. From the code I wrote below, it seems like they like warmer temperatures on average in the summer. \n\n> Their distance to the coast is also very important in determining the prescence of this eel species. The average distance for the dataset while the eel is present is HALF of that with the eel absent, so I'm thinking that the eel likes to be closer to the coast rather than farther. Maybe this is because they are anadromous in some way or need brackish water for part of their life cycle.\n\n> The species also seems to be heavily influence in the amount of native forest that the particular habitat contains... no surprise there! \n\n> The species also seems to like areas with more gently sloped downstream areas, which to me suggests that they somewhat rely on being able to travel up and downstream... \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a little subset dataframe for just the places that the eel was present\neel_pres <- eel_dat |> \n  filter(Angaus == 1)\neel_abs <- eel_dat |> \n  filter(Angaus == 0)\n\n# find out what summer temperature they like \nmean(eel_pres$SegSumT) # = 17.8005\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17.8005\n```\n:::\n\n```{.r .cell-code}\nmean(eel_abs$SegSumT) # = 16.8005\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16.07343\n```\n:::\n\n```{.r .cell-code}\n# find out if they like to be closer or farther from the coast\nmean(eel_pres$DSDist) # = 42.47604\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 42.47604\n```\n:::\n\n```{.r .cell-code}\nmean(eel_abs$DSDist) # = 82.60588\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 82.60588\n```\n:::\n\n```{.r .cell-code}\n# find out whether they like steeper slope or shallower slope \nmean(eel_pres$DSMaxSlope) # = 1.544\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.544554\n```\n:::\n\n```{.r .cell-code}\nmean(eel_abs$DSMaxSlope) # = 3.395\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.395376\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}